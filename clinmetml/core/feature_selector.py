"""
Feature Selection Module for Auto Pipeline
Converts R-based sampling and matching logic to Python
"""

import pandas as pd
import numpy as np
from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import ElasticNet
import os
from pathlib import Path
from typing import Optional, Tuple, List, Dict, Set
from collections import Counter, defaultdict
import glob

# ClinMetML path management
from ..utils.paths import (
    get_feature_selection_dir, 
    get_feature_selection_subdir
)

# å¯é€‰ä¾èµ–
try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
    print("Warning: lightgbm not available. LightGBM method will be disabled.")

try:
    import pymrmr
    PYMRMR_AVAILABLE = True
except ImportError:
    PYMRMR_AVAILABLE = False
    print("Warning: pymrmr not available. mRMR method will be disabled.")

try:
    from skrebate import ReliefF
    RELIEF_AVAILABLE = True
except ImportError:
    RELIEF_AVAILABLE = False
    print("Warning: skrebate not available. ReliefF method will be disabled.")

try:
    from skfeature.function.information_theoretical_based import FCBF
    FCBF_AVAILABLE = True
    FCBF_TYPE = 'skfeature'
except ImportError:
    try:
        from fcbf import fcbf
        FCBF_AVAILABLE = True
        FCBF_TYPE = 'fcbf'
    except ImportError:
        try:
            from FCBF_module import FCBF
            FCBF_AVAILABLE = True
            FCBF_TYPE = 'FCBF_module'
        except ImportError:
            FCBF_AVAILABLE = False
            FCBF_TYPE = None
            print("Warning: FCBF module not available. FCBF method will be disabled.")


def analyze_dataset_balance(data_path: str, target_col: str, balance_threshold: float = 0.3) -> Dict[str, any]:
    """
    åˆ†ææ•°æ®é›†çš„å¹³è¡¡æ€§
    
    Args:
        data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        target_col: ç›®æ ‡åˆ—å
        balance_threshold: å¹³è¡¡é˜ˆå€¼ï¼Œå¦‚æœå°‘æ•°ç±»å æ¯”å°äºæ­¤å€¼åˆ™è®¤ä¸ºä¸å¹³è¡¡
        
    Returns:
        Dict: åŒ…å«æ•°æ®é›†åˆ†æç»“æœçš„å­—å…¸
    """
    print("ğŸ” åˆ†ææ•°æ®é›†å¹³è¡¡æ€§...")
    print("=" * 50)
    
    try:
        # è¯»å–æ•°æ®
        df = pd.read_csv(data_path)
        
        if target_col not in df.columns:
            raise ValueError(f"ç›®æ ‡åˆ— '{target_col}' ä¸å­˜åœ¨äºæ•°æ®ä¸­")
        
        # ç»Ÿè®¡ç›®æ ‡å˜é‡åˆ†å¸ƒ
        target_counts = df[target_col].value_counts().sort_index()
        total_samples = len(df)
        
        print(f"ğŸ“Š æ•°æ®é›†åŸºæœ¬ä¿¡æ¯:")
        print(f"   æ€»æ ·æœ¬æ•°: {total_samples}")
        print(f"   ç‰¹å¾æ•°: {len(df.columns) - 1}")
        print(f"   ç›®æ ‡åˆ—: {target_col}")
        
        print(f"\nğŸ“ˆ ç›®æ ‡å˜é‡åˆ†å¸ƒ:")
        for value, count in target_counts.items():
            percentage = (count / total_samples) * 100
            print(f"   ç±»åˆ« {value}: {count} æ ·æœ¬ ({percentage:.1f}%)")
        
        # åˆ¤æ–­æ•°æ®é›†å¹³è¡¡æ€§
        min_class_ratio = target_counts.min() / total_samples
        is_balanced = min_class_ratio >= balance_threshold
        
        # è®¡ç®—ä¸å¹³è¡¡æ¯”ä¾‹
        max_count = target_counts.max()
        min_count = target_counts.min()
        imbalance_ratio = max_count / min_count
        
        print(f"\nâš–ï¸ å¹³è¡¡æ€§åˆ†æ:")
        print(f"   å°‘æ•°ç±»å æ¯”: {min_class_ratio:.3f}")
        print(f"   ä¸å¹³è¡¡æ¯”ä¾‹: {imbalance_ratio:.2f}:1")
        print(f"   å¹³è¡¡é˜ˆå€¼: {balance_threshold}")
        
        if is_balanced:
            print(f"   âœ… æ•°æ®é›†ç›¸å¯¹å¹³è¡¡ (å°‘æ•°ç±»å æ¯” >= {balance_threshold})")
            recommended_strategy = "balanced"
        else:
            print(f"   âš ï¸ æ•°æ®é›†ä¸å¹³è¡¡ (å°‘æ•°ç±»å æ¯” < {balance_threshold})")
            recommended_strategy = "imbalanced"
        
        print(f"\nğŸ’¡ æ¨èç­–ç•¥: {recommended_strategy}")
        
        return {
            'total_samples': total_samples,
            'feature_count': len(df.columns) - 1,
            'target_distribution': target_counts.to_dict(),
            'min_class_ratio': min_class_ratio,
            'imbalance_ratio': imbalance_ratio,
            'is_balanced': is_balanced,
            'recommended_strategy': recommended_strategy,
            'balance_threshold': balance_threshold
        }
        
    except Exception as e:
        print(f"âŒ åˆ†ææ•°æ®é›†æ—¶å‡ºé”™: {e}")
        raise


def run_balanced_feature_selection(data_path: str,
                                 target_col: str,
                                 methods: List[str] = None,
                                 top_k: int = 50,
                                 output_dir: str = "balanced_feature_selection",
                                 id_col: str = None) -> Dict[str, str]:
    """
    å¯¹å¹³è¡¡æ•°æ®é›†è¿›è¡Œç‰¹å¾é€‰æ‹©ï¼ˆä¸éœ€è¦é‡é‡‡æ ·ï¼‰
    
    Args:
        data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        target_col: ç›®æ ‡åˆ—å
        methods: ç‰¹å¾é€‰æ‹©æ–¹æ³•åˆ—è¡¨
        top_k: é€‰æ‹©çš„ç‰¹å¾æ•°é‡
        output_dir: è¾“å‡ºç›®å½•
        id_col: IDåˆ—å
        
    Returns:
        Dict: å„æ–¹æ³•çš„ç»“æœæ–‡ä»¶è·¯å¾„
    """
    print("\nğŸ¯ å¼€å§‹å¹³è¡¡æ•°æ®é›†ç‰¹å¾é€‰æ‹©...")
    print("=" * 60)
    
    if methods is None:
        methods = ['randomforest', 'lightgbm', 'elasticnet', 'fcbf', 'relief']
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½æ•°æ®
    print(f"ğŸ“ åŠ è½½æ•°æ®: {data_path}")
    X, y, feature_labels = acquire_data_for_feature_selection(data_path, target_col, id_col)
    
    print(f"ğŸ“Š æ•°æ®ç»´åº¦: {X.shape}")
    print(f"ğŸ¯ ç›®æ ‡åˆ†å¸ƒ: {y.value_counts().to_dict()}")
    
    # å¯ç”¨çš„æ–¹æ³•å‡½æ•°æ˜ å°„
    method_functions = {
        'lightgbm': lightgbm_feature_selection,
        'randomforest': randomforest_feature_selection,
        'elasticnet': elasticnet_feature_selection,
        'fcbf': fcbf_feature_selection,
        'relief': relief_feature_selection
    }
    
    # è¿‡æ»¤å¯ç”¨æ–¹æ³•
    available_methods = []
    for method in methods:
        if method == 'lightgbm' and not LIGHTGBM_AVAILABLE:
            print(f"âš ï¸ è·³è¿‡ {method}: LightGBM ä¸å¯ç”¨")
            continue
        elif method == 'fcbf' and not FCBF_AVAILABLE:
            print(f"âš ï¸ è·³è¿‡ {method}: FCBF ä¸å¯ç”¨")
            continue
        elif method == 'relief' and not RELIEF_AVAILABLE:
            print(f"âš ï¸ è·³è¿‡ {method}: ReliefF ä¸å¯ç”¨")
            continue
        elif method == 'mrmr' and not PYMRMR_AVAILABLE:
            print(f"âš ï¸ è·³è¿‡ {method}: mRMR ä¸å¯ç”¨")
            continue
        elif method in method_functions or method == 'mrmr':
            available_methods.append(method)
        else:
            print(f"âš ï¸ è·³è¿‡ {method}: æœªçŸ¥æ–¹æ³•")
    
    if not available_methods:
        raise ValueError("æ²¡æœ‰å¯ç”¨çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•")
    
    print(f"âœ… å¯ç”¨æ–¹æ³•: {available_methods}")
    
    # æ‰§è¡Œç‰¹å¾é€‰æ‹©
    results = {}
    
    for method in available_methods:
        print(f"\nğŸ”„ æ‰§è¡Œ {method.upper()} ç‰¹å¾é€‰æ‹©...")
        
        try:
            if method == 'mrmr':
                # mRMRéœ€è¦å®Œæ•´æ•°æ®æ¡†
                df = pd.read_csv(data_path)
                if id_col and id_col in df.columns:
                    df = df.drop(id_col, axis=1)
                elif not id_col:
                    df = df.drop(df.columns[0], axis=1)
                
                result = mrmr_feature_selection(df, target_col, 1, top_k)
            else:
                # å…¶ä»–æ–¹æ³•
                result = method_functions[method](X, y, feature_labels, 1, top_k)
            
            # ä¿å­˜ç»“æœ
            output_file = output_path / f"{method}_features.csv"
            save_feature_selection_results(result, str(output_file))
            results[method] = str(output_file)
            
            # æ˜¾ç¤ºé€‰æ‹©çš„ç‰¹å¾æ•°é‡
            if isinstance(result, pd.DataFrame):
                feature_count = len(result)
            elif isinstance(result, pd.Series):
                feature_count = len(result)
            else:
                feature_count = "æœªçŸ¥"
            
            print(f"   âœ… {method}: é€‰æ‹©äº† {feature_count} ä¸ªç‰¹å¾")
            print(f"   ğŸ’¾ ä¿å­˜åˆ°: {output_file}")
            
        except Exception as e:
            print(f"   {method} æ‰§è¡Œå¤±è´¥: {e}")
            continue
    
    print(f"\n å¹³è¡¡æ•°æ®é›†ç‰¹å¾é€‰æ‹©å®Œæˆ!")
    print(f" ç»“æœä¿å­˜åœ¨: {output_dir}")
    
    return results


def run_auto_feature_selection(data_path: str,
                               target_col: str,
                               methods: List[str] = None,
                               top_k: int = 50,
                               output_dir: Optional[str] = None,
                               id_col: Optional[str] = None,
                               balance_threshold: float = 0.3,
                               n_iterations: int = 100,
                               sample_ratio: float = 0.75,
                               match_ratio: int = 3,
                               random_state: int = 123,
                               match_cols: Optional[List[str]] = None,
                               run_robust_analysis: bool = True,
                               robust_threshold: float = 0.5,
                               run_feature_voting: bool = True,
                               run_final_dataset: bool = True,
                               final_min_features: int = 10,
                               final_covariates: Optional[List[str]] = None) -> Dict[str, str]:
    """æ ¹æ®æ•°æ®åˆ†å¸ƒè‡ªåŠ¨é€‰æ‹©å¹³è¡¡æˆ–é‡é‡‡æ ·ç‰¹å¾é€‰æ‹©ç­–ç•¥ã€‚

    å½“æ•°æ®é›†ç›¸å¯¹å¹³è¡¡æ—¶ï¼Œè°ƒç”¨ ``run_balanced_feature_selection``ï¼›
    å½“æ•°æ®é›†ä¸å¹³è¡¡æ—¶ï¼Œè‡ªåŠ¨æ¨æ–­åŒ¹é…å˜é‡å¹¶è°ƒç”¨é‡é‡‡æ ·æµæ°´çº¿ï¼š
        ``generate_resampled_datasets`` + ``run_feature_selection_methods``ã€‚
    """

    # åˆ†ææ•°æ®é›†å¹³è¡¡æ€§
    balance_info = analyze_dataset_balance(
        data_path=data_path,
        target_col=target_col,
        balance_threshold=balance_threshold,
    )

    strategy = balance_info.get("recommended_strategy", "balanced")
    print(f"\n Auto feature selection strategy: {strategy}")

    # å¹³è¡¡æ•°æ®ï¼šç›´æ¥èµ°åŸæœ‰çš„å¹³è¡¡ç‰¹å¾é€‰æ‹©æµç¨‹
    if strategy == "balanced":
        return run_balanced_feature_selection(
            data_path=data_path,
            target_col=target_col,
            methods=methods,
            top_k=top_k,
            output_dir=output_dir or get_feature_selection_subdir("balanced"),
            id_col=id_col,
        )

    # ä¸å¹³è¡¡æ•°æ®ï¼šä½¿ç”¨é‡é‡‡æ ·ç‰¹å¾é€‰æ‹©
    print("\n æ£€æµ‹åˆ°æ•°æ®é›†ä¸å¹³è¡¡ï¼Œå¯ç”¨é‡é‡‡æ ·ç‰¹å¾é€‰æ‹©æµæ°´çº¿...")

    # åŒ¹é…å˜é‡è®¾ç½®ï¼š
    # - å¦‚æœç”¨æˆ·æ˜¾å¼æä¾› match_colsï¼Œåˆ™ä½¿ç”¨ç”¨æˆ·æä¾›çš„åˆ—ï¼ˆå¹¶æ£€æŸ¥æ˜¯å¦å­˜åœ¨ï¼‰ï¼›
    # - å¦‚æœæœªæä¾›ï¼Œåˆ™ä½¿ç”¨é»˜è®¤ ['age', 'bmi', 'gender']ï¼Œåªä¿ç•™åœ¨æ•°æ®ä¸­çœŸå®å­˜åœ¨çš„åˆ—ã€‚
    try:
        df = pd.read_csv(data_path)
        available_cols = set(df.columns)

        if match_cols is None:
            default_match_cols = ["age", "bmi", "gender"]
            match_cols = [c for c in default_match_cols if c in available_cols]
            if not match_cols:
                raise ValueError(
                    "æœªèƒ½åœ¨æ•°æ®ä¸­æ‰¾åˆ°ä»»ä½•é»˜è®¤åŒ¹é…å˜é‡ ['age', 'bmi', 'gender']ï¼Œ"
                    "è¯·åœ¨ run_auto_feature_selection ä¸­æ˜¾å¼è®¾ç½® match_cols å‚æ•°ã€‚"
                )
            print(
                f"Using default match columns (count={len(match_cols)}): "
                f"{match_cols}"
            )
        else:
            # ç”¨æˆ·è‡ªå®šä¹‰åŒ¹é…å˜é‡ï¼šåªä¿ç•™çœŸå®å­˜åœ¨çš„åˆ—
            original_match_cols = list(match_cols)
            match_cols = [c for c in original_match_cols if c in available_cols]
            if not match_cols:
                raise ValueError(
                    f"æä¾›çš„åŒ¹é…å˜é‡ {original_match_cols} åœ¨æ•°æ®ä¸­å‡ä¸å­˜åœ¨ï¼Œ"
                    "è¯·æ£€æŸ¥åˆ—åæˆ–é‡æ–°è®¾ç½® match_colsã€‚"
                )
            missing = [c for c in original_match_cols if c not in available_cols]
            if missing:
                print(f" ä»¥ä¸‹åŒ¹é…å˜é‡åœ¨æ•°æ®ä¸­æœªæ‰¾åˆ°ï¼Œå°†è¢«å¿½ç•¥: {missing}")
            print(
                f"Using user-specified match columns (count={len(match_cols)}): "
                f"{match_cols}"
            )
    except Exception as e:
        print(f" åŒ¹é…å˜é‡é…ç½®å¤±è´¥: {e}")
        raise

    # ç”Ÿæˆé‡é‡‡æ ·æ•°æ®é›†
    _ = generate_resampled_datasets(
        data_path=data_path,
        n_iterations=n_iterations,
        target_col=target_col,
        match_cols=match_cols,
        sample_ratio=sample_ratio,
        match_ratio=match_ratio,
        random_state=random_state,
    )

    resampling_dir = get_feature_selection_subdir("resampling")

    # åœ¨é‡é‡‡æ ·æ•°æ®ä¸Šè¿è¡Œç‰¹å¾é€‰æ‹©æ–¹æ³•
    fs_summary = run_feature_selection_methods(
        input_dir=resampling_dir,
        target_col=target_col,
        iterations=n_iterations,
        methods=methods,
        top_k=top_k,
        output_base_dir=output_dir or get_feature_selection_dir(),
        id_col=id_col,
    )

    # run_feature_selection_methods è¿”å›çš„æ˜¯æ‘˜è¦ï¼Œè¿™é‡Œå°†å…¶è½¬æ¢ä¸ºä¸
    # run_balanced_feature_selection ç±»ä¼¼çš„è¿”å›å½¢å¼ï¼šæ–¹æ³•å -> è¾“å‡ºç›®å½•
    results: Dict[str, str] = {}
    for method, info in fs_summary.items():
        if isinstance(info, dict) and "output_dir" in info:
            results[method] = info["output_dir"]
        else:
            results[method] = str(info)

    base_results_dir = output_dir or get_feature_selection_dir()

    # å¯é€‰ï¼šç¨³å¥ç‰¹å¾åˆ†æ
    robust_results = None
    if run_robust_analysis:
        robust_output_dir = os.path.join(base_results_dir, "robust_features_analysis")
        robust_results = analyze_robust_features(
            results_base_dir=base_results_dir,
            threshold=robust_threshold,
            output_dir=robust_output_dir,
        )

    # å¯é€‰ï¼šç‰¹å¾æŠ•ç¥¨åˆ†æï¼ˆä¾èµ–ç¨³å¥ç‰¹å¾åˆ†æï¼‰
    voting_results = None
    voting_output_dir = None
    if run_feature_voting:
        voting_output_dir = os.path.join(base_results_dir, "feature_voting_analysis")
        voting_results = analyze_feature_voting(
            results_base_dir=base_results_dir,
            threshold=robust_threshold,
            output_dir=voting_output_dir,
        )

    # å¯é€‰ï¼šæ ¹æ®æŠ•ç¥¨ç»“æœåˆ›å»ºæœ€ç»ˆç‰¹å¾æ•°æ®é›†
    if run_final_dataset:
        analysis_dir = voting_output_dir or os.path.join(base_results_dir, "feature_voting_analysis")
        final_output_path = os.path.join(base_results_dir, "final_feature_dataset.csv")
        _ = create_final_feature_dataset(
            original_data_path=data_path,
            analysis_dir=analysis_dir,
            output_path=final_output_path,
            min_features=final_min_features,
            covariates=final_covariates,
            target_col=target_col,
            id_col=id_col,
        )

    print("\n è‡ªåŠ¨ç‰¹å¾é€‰æ‹©ï¼ˆå«é‡é‡‡æ ·ã€ç¨³å¥ç‰¹å¾ä¸æŠ•ç¥¨åˆ†æï¼‰å®Œæˆ!")
    print(f" ç»“æœåŸºç¡€ç›®å½•: {base_results_dir}")

    return results


def sample_extract(data: pd.DataFrame, 
                  target_col: str,
                  match_cols: list,
                  sample_ratio: float,
                  match_ratio: int,
                  random_state: int) -> pd.DataFrame:
    """
    Extract matched samples from data using propensity score matching
    
    Args:
        data: Input DataFrame
        target_col: Target column name for matching
        match_cols: Columns to use for matching
        sample_ratio: Ratio of data to sample
        match_ratio: Matching ratio
        random_state: Random seed for reproducibility
        
    Returns:
        DataFrame with matched samples
    """
        
    np.random.seed(random_state)
    
    # 1. Random sampling (equivalent to R's sample())
    n_samples = int(len(data) * sample_ratio)
    sample_indices = np.random.choice(data.index, size=n_samples, replace=False)
    data_sample = data.loc[sample_indices].copy()
    
    # 2. Propensity score matching using nearest neighbors
    # Separate treatment and control groups
    treatment_group = data_sample[data_sample[target_col] == 1]
    control_group = data_sample[data_sample[target_col] == 0]
    
    if len(treatment_group) == 0 or len(control_group) == 0:
        print("Warning: One of the groups is empty after sampling")
        return data_sample
    
    # Standardize matching variables
    scaler = StandardScaler()
    
    # Fit scaler on all data and transform both groups
    all_match_data = data_sample[match_cols].fillna(data_sample[match_cols].mean())
    scaler.fit(all_match_data)
    
    treatment_features = scaler.transform(
        treatment_group[match_cols].fillna(treatment_group[match_cols].mean())
    )
    control_features = scaler.transform(
        control_group[match_cols].fillna(control_group[match_cols].mean())
    )
    
    # Use KNN for matching (equivalent to R's matchit with nearest neighbor)
    nn = NearestNeighbors(n_neighbors=min(match_ratio, len(control_group)), 
                         metric='euclidean')
    nn.fit(control_features)
    
    # Find matches for each treatment case
    matched_indices = []
    matched_indices.extend(treatment_group.index.tolist())  # Include all treatment cases
    
    distances, indices = nn.kneighbors(treatment_features)
    
    # Add matched control cases
    for i, treatment_idx in enumerate(treatment_group.index):
        for j in range(min(match_ratio, len(indices[i]))):
            control_idx = control_group.index[indices[i][j]]
            if control_idx not in matched_indices:  # Avoid duplicates
                matched_indices.append(control_idx)
    
    # Create matched dataset
    matched_data = data_sample.loc[matched_indices].copy()
    
    # 3. Remove last 3 columns (equivalent to R's [-c(92,93,94)])
    # Note: This removes the last 3 columns regardless of their position
    if len(matched_data.columns) >= 3:
        matched_data = matched_data.iloc[:, :-3]
    
    return matched_data


def check_data_columns(data_path: str, target_col: str, match_cols: list) -> bool:
    """
    Check if the specified columns exist in the data
    
    Args:
        data_path: Path to data file
        target_col: Target column name
        match_cols: List of matching column names
        
    Returns:
        True if all columns exist, False otherwise
    """
    try:
        data = pd.read_csv(data_path)
        print(f"Data shape: {data.shape}")
        print(f"Available columns: {list(data.columns)[:10]}...")  # Show first 10 columns
        
        missing_cols = []
        
        # Check target column
        if target_col not in data.columns:
            missing_cols.append(target_col)
        else:
            print(f"âœ“ Target column '{target_col}' found")
            print(f"  Value counts: {data[target_col].value_counts().to_dict()}")
        
        # Check match columns
        for col in match_cols:
            if col not in data.columns:
                missing_cols.append(col)
            else:
                print(f"âœ“ Match column '{col}' found")
        
        if missing_cols:
            print(f"âŒ Missing columns: {missing_cols}")
            print("Available columns:")
            for i, col in enumerate(data.columns):
                print(f"  {i+1:2d}. {col}")
            return False
        
        return True
        
    except Exception as e:
        print(f"âŒ Error checking data: {e}")
        return False


def generate_resampled_datasets(data_path: str,
                              n_iterations: int,
                              target_col: str,
                              match_cols: list,
                              sample_ratio: float,
                              match_ratio: int,
                              random_state: int) -> list:
    """
    Generate multiple resampled and matched datasets for PLSDA analysis
    
    Args:
        data_path: Path to input CSV file
        n_iterations: Number of iterations
        target_col: Target column for matching
        match_cols: Columns to use for matching
        sample_ratio: Sampling ratio
        match_ratio: Matching ratio
        random_state: Base random seed
        
    Returns:
        List of file paths for generated datasets
        
    Note:
        Output directory is managed by ClinMetML path manager
    """
        
    # Read data
    if not os.path.exists(data_path):
        raise FileNotFoundError(f"Data file not found: {data_path}")
    
    data = pd.read_csv(data_path)
    print(f"Loaded data with shape: {data.shape}")
    
    # Create output directory using path manager
    output_path = Path(get_feature_selection_subdir("resampling"))
    
    # Store results
    result_files = []
    
    print(f"Generating {n_iterations} resampled datasets...")
    
    for i in range(1, n_iterations + 1):
        try:
            # Use different random state for each iteration
            current_seed = random_state + i
            
            # Extract matched sample
            data_sample = sample_extract(
                data=data,
                target_col=target_col,
                match_cols=match_cols,
                sample_ratio=sample_ratio,
                match_ratio=match_ratio,
                random_state=current_seed
            )
            
            # Save file
            file_name = f"matched_Data_test{i}.csv"
            file_path = output_path / file_name
            data_sample.to_csv(file_path, index=False)
            result_files.append(str(file_path))
            
            # Progress indicator
            if i % 100 == 0:
                print(f"Completed {i}/{n_iterations} iterations")
                
        except Exception as e:
            print(f"Error in iteration {i}: {e}")
            continue
    
    print(f"âœ“ Generated {len(result_files)} datasets in 'resampling' directory")
    return result_files


def run_feature_selection_pipeline(data_path: str,
                                 n_iterations: int,
                                 target_col: str,
                                 match_cols: list,
                                 sample_ratio: float = 0.75,
                                 match_ratio: int = 3,
                                 random_state: int = 123) -> bool:
    """
    Main function to run the complete feature selection pipeline
    
    Args:
        data_path: Path to input data file
        n_iterations: Number of resampling iterations
        target_col: Target column for matching
        match_cols: Columns to use for matching
        sample_ratio: Sampling ratio (default: 0.75)
        match_ratio: Matching ratio (default: 3)
        random_state: Random seed (default: 123)
        
    Returns:
        True if successful, False otherwise
        
    Note:
        Output directory is managed by ClinMetML path manager
    """
        
    try:
        print("="*60)
        print("Feature Selection Pipeline")
        print("="*60)
        
        # Check if data file exists
        if not os.path.exists(data_path):
            print(f"âŒ Data file not found: {data_path}")
            return False
        
        # Check data columns
        print(f"\nChecking data columns...")
        if not check_data_columns(data_path, target_col, match_cols):
            return False
        
        # Generate resampled datasets
        result_files = generate_resampled_datasets(
            data_path=data_path,
            n_iterations=n_iterations,
            target_col=target_col,
            match_cols=match_cols,
            sample_ratio=sample_ratio,
            match_ratio=match_ratio,
            random_state=random_state
        )
        
        if len(result_files) > 0:
            print(f"\nâœ… Pipeline completed successfully!")
            print(f"Generated {len(result_files)} matched datasets")
            print(f"Output directory: {get_feature_selection_subdir('resampling')}")
            return True
        else:
            print(f"\nâŒ Pipeline failed - no datasets generated")
            return False
            
    except Exception as e:
        print(f"âŒ Pipeline error: {e}")
        return False


class FeatureSelector:
    """High-level feature selection interface used by ClinMetMLPipeline.

    This class provides a `select_features` method compatible with the
    pipeline configuration shown in the README and examples, but internally
    it reuses the `run_auto_feature_selection` pipeline implemented above.
    """

    def __init__(self):
        self.last_results_dir: Optional[str] = None

    def select_features(
        self,
        data: pd.DataFrame,
        target_column: str,
        **kwargs,
    ) -> pd.DataFrame:
        """Select features and return a DataFrame containing selected features.

        Parameters
        ----------
        data : pd.DataFrame
            Input dataframe including the target column.
        target_column : str
            Name of the target column in `data`.
        **kwargs : dict
            Additional configuration, typically coming from
            `pipeline_config['feature_selection']`, e.g.:

            - method: list[str] or str
            - k_best: mapped to `top_k`
            - output_dir: base directory for feature selection outputs
            - id_column: identifier column name (mapped to `id_col`)
            - other arguments accepted by `run_auto_feature_selection`.
        """

        # Resolve output directory
        output_dir = kwargs.get("output_dir")
        if output_dir is None:
            output_dir = get_feature_selection_dir()
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        # Persist current data to CSV for the file-based APIs
        input_csv = output_path / "feature_selection_input.csv"
        data.to_csv(input_csv, index=False)

        # Map generic pipeline params to run_auto_feature_selection arguments
        methods = kwargs.get("method")
        # Allow method to be a single string or a list
        if isinstance(methods, str):
            methods_arg = [methods]
        else:
            methods_arg = methods

        top_k = kwargs.get("k_best", kwargs.get("top_k", 50))
        id_col = kwargs.get("id_column", kwargs.get("id_col"))

        # Optional advanced parameters with sensible defaults
        balance_threshold = kwargs.get("balance_threshold", 0.3)
        n_iterations = kwargs.get("n_iterations", 3)
        sample_ratio = kwargs.get("sample_ratio", 0.75)
        match_ratio = kwargs.get("match_ratio", 3)
        random_state = kwargs.get("random_state", 123)
        match_cols = kwargs.get("match_cols")
        final_min_features = kwargs.get("final_min_features", 10)
        final_covariates = kwargs.get("final_covariates")

        results = run_auto_feature_selection(
            data_path=str(input_csv),
            target_col=target_column,
            methods=methods_arg,
            top_k=top_k,
            output_dir=str(output_path),
            id_col=id_col,
            balance_threshold=balance_threshold,
            n_iterations=n_iterations,
            sample_ratio=sample_ratio,
            match_ratio=match_ratio,
            random_state=random_state,
            match_cols=match_cols,
            run_robust_analysis=True,
            robust_threshold=kwargs.get("robust_threshold", 0.5),
            run_feature_voting=True,
            run_final_dataset=True,
            final_min_features=final_min_features,
            final_covariates=final_covariates,
        )

        # By convention, run_auto_feature_selection will write a
        # "final_feature_dataset.csv" into the base results directory.
        self.last_results_dir = str(output_path)
        final_dataset_path = output_path / "final_feature_dataset.csv"

        if not final_dataset_path.exists():
            raise FileNotFoundError(
                f"Expected final feature dataset at '{final_dataset_path}', "
                "but the file was not found. Please check feature selection outputs."
            )

        selected_data = pd.read_csv(final_dataset_path)
        return selected_data


# ==================== ç‰¹å¾é€‰æ‹©æ–¹æ³• ====================

def acquire_data_for_feature_selection(file_path: str, target_col: str, id_col: str = None) -> tuple:
    """
    åŠ è½½æ•°æ®å¹¶å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾ï¼ˆç”¨äºç‰¹å¾é€‰æ‹©ï¼‰
    
    Args:
        file_path: CSVæ–‡ä»¶è·¯å¾„
        target_col: ç›®æ ‡åˆ—åï¼ˆç”¨æˆ·å®šä¹‰çš„target-colï¼‰
        id_col: IDåˆ—åï¼ˆå¦‚æœä¸ºNoneï¼Œè‡ªåŠ¨åˆ é™¤ç¬¬ä¸€åˆ—ï¼‰
        
    Returns:
        tuple: (X, y, feature_labels)
    """
    try:
        # ä½¿ç”¨æ›´é«˜æ•ˆçš„è¯»å–æ–¹å¼ï¼ŒæŒ‡å®šæ•°æ®ç±»å‹ä»¥èŠ‚çœå†…å­˜
        df = pd.read_csv(file_path, low_memory=False, dtype='float32')
        
        # åˆ é™¤IDåˆ—
        if id_col and id_col in df.columns:
            df = df.drop(id_col, axis=1)
        elif not id_col:
            # åˆ é™¤ç¬¬ä¸€åˆ—ï¼ˆå‡è®¾æ˜¯IDåˆ—ï¼‰
            df = df.drop(df.columns[0], axis=1)
        
        # æå–ç›®æ ‡å˜é‡å’Œç‰¹å¾
        if target_col not in df.columns:
            raise ValueError(f"Target column '{target_col}' not found in data")
        
        # ç›´æ¥æå–ï¼Œé¿å…é¢å¤–çš„copyæ“ä½œ
        y = df[target_col].astype('int32')
        X = df.drop(target_col, axis=1)
        feature_labels = X.columns.values
        
        # æ¸…ç†å†…å­˜
        del df
        
        return X, y, feature_labels
        
    except Exception as e:
        print(f"Error loading {file_path}: {e}")
        raise


def lightgbm_feature_selection(X: pd.DataFrame, y: pd.Series, 
                              feature_labels: np.ndarray, 
                              iteration: int,
                              top_k: int = 50,
                              test_size: float = 0.3,
                              random_state: int = 42) -> pd.DataFrame:
    """
    ä½¿ç”¨LightGBMè¿›è¡Œç‰¹å¾é€‰æ‹©
    """
    if not LIGHTGBM_AVAILABLE:
        raise ImportError("LightGBM not available")
        
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state
    )
    
    params = {
        'objective': 'binary',
        'metric': 'auc',
        'num_leaves': 31,
        'learning_rate': 0.05,
        'verbose': -1
    }
    
    lgb_train = lgb.Dataset(X_train, y_train)
    model = lgb.train(params, lgb_train, num_boost_round=100)
    
    feature_importance = model.feature_importance(importance_type='gain')
    indices = feature_importance.argsort()[::-1]
    top_indices = indices[:top_k]
    
    importances_df = pd.DataFrame({
        'feature': feature_labels[top_indices],
        'importance': feature_importance[top_indices]
    })
    
    print(f"LightGBM - å·²å®Œæˆ: {iteration}")
    return importances_df


def randomforest_feature_selection(X: pd.DataFrame, y: pd.Series,
                                  feature_labels: np.ndarray,
                                  iteration: int,
                                  top_k: int = 50,
                                  test_size: float = 0.3,
                                  random_state: int = 42,
                                  n_estimators: int = 1000) -> pd.DataFrame:
    """
    ä½¿ç”¨éšæœºæ£®æ—è¿›è¡Œç‰¹å¾é€‰æ‹©
    """
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state
    )
    
    forest = RandomForestClassifier(
        n_estimators=n_estimators, 
        random_state=random_state, 
        n_jobs=-1
    )
    forest.fit(X_train, y_train)
    
    importances = forest.feature_importances_
    indices = np.argsort(importances)[::-1]
    top_indices = indices[:top_k]
    
    importances_df = pd.DataFrame({
        'feature': feature_labels[top_indices],
        'importance': importances[top_indices]
    })
    
    print(f"RandomForest - å·²å®Œæˆ: {iteration}")
    return importances_df


def elasticnet_feature_selection(X: pd.DataFrame, y: pd.Series,
                                feature_labels: np.ndarray,
                                iteration: int,
                                top_k: int = 50,
                                test_size: float = 0.3,
                                random_state: int = 42,
                                alpha: float = 0.005,
                                l1_ratio: float = 0.1) -> pd.DataFrame:
    """
    ä½¿ç”¨ElasticNetè¿›è¡Œç‰¹å¾é€‰æ‹©
    """
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state
    )
    
    enet = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=random_state)
    enet.fit(X_train, y_train)
    
    coef = enet.coef_
    importances = np.abs(coef)
    indices = np.argsort(importances)[::-1]
    top_indices = indices[:top_k]
    
    importances_df = pd.DataFrame({
        'feature': feature_labels[top_indices],
        'importance': importances[top_indices]
    })
    
    print(f"ElasticNet - å·²å®Œæˆ: {iteration}")
    return importances_df


def fcbf_feature_selection(X: pd.DataFrame, y: pd.Series,
                          feature_labels: np.ndarray,
                          iteration: int,
                          top_k: int = 50,
                          test_size: float = 0.3,
                          random_state: int = 42) -> pd.DataFrame:
    """
    ä½¿ç”¨FCBFè¿›è¡Œç‰¹å¾é€‰æ‹©
    """
    if not FCBF_AVAILABLE:
        raise ImportError("FCBF module not available")
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state
    )
    
    X_train_array = np.array(X_train)
    y_train_array = np.array(y_train)
    
    # æ ¹æ®å¯ç”¨çš„FCBFåŒ…ç±»å‹é€‰æ‹©ä½¿ç”¨æ–¹å¼
    if FCBF_TYPE == 'skfeature':
        # ä½¿ç”¨ skfeature çš„ FCBF
        # ä¸´æ—¶ä¿®å¤NumPyå…¼å®¹æ€§é—®é¢˜
        original_zeros = np.zeros
        def patched_zeros(shape, dtype=float, order='C', **kwargs):
            # å¤„ç†å¯èƒ½çš„dtypeså‚æ•°ï¼ˆæŸäº›ç‰ˆæœ¬å¯èƒ½ä½¿ç”¨dtypesè€Œä¸æ˜¯dtypeï¼‰
            if 'dtypes' in kwargs:
                dtype = kwargs.pop('dtypes')
            return original_zeros(shape, dtype=dtype, order=order, **kwargs)
        np.zeros = patched_zeros
        
        try:
            from skfeature.function.information_theoretical_based import FCBF
            result = FCBF.fcbf(X_train_array, y_train_array, n_selected_features=top_k)
        finally:
            # æ¢å¤åŸå§‹å‡½æ•°
            np.zeros = original_zeros
        
        # FCBFè¿”å›(selected_features, selected_feature_scores)
        if isinstance(result, tuple) and len(result) == 2:
            idx, scores = result
            # ç¡®ä¿ç´¢å¼•æ˜¯æ•´æ•°ç±»å‹
            idx = [int(i) for i in idx[:top_k]]
        else:
            # å¦‚æœè¿”å›æ ¼å¼ä¸åŒï¼Œä½¿ç”¨å‰top_kä¸ªç‰¹å¾ä½œä¸ºfallback
            idx = list(range(min(top_k, len(feature_labels))))
        
        selected_features = feature_labels[idx]
        importances_df = pd.DataFrame({'feature': selected_features})
        
    elif FCBF_TYPE == 'fcbf':
        # ä½¿ç”¨ fcbf åŒ…
        from fcbf import fcbf
        # fcbfåŒ…éœ€è¦DataFrameå’ŒSeriesï¼Œè¿”å›ç›¸å…³ç‰¹å¾åˆ—è¡¨
        X_train_df = pd.DataFrame(X_train, columns=feature_labels)
        relevant_features, irrelevant_features, correlations = fcbf(X_train_df, y_train)
        
        # å–å‰top_kä¸ªç‰¹å¾
        selected_features = relevant_features[:top_k]
        importances_df = pd.DataFrame({'feature': selected_features})
        
    elif FCBF_TYPE == 'FCBF_module':
        # ä½¿ç”¨ FCBF_module åŒ…
        # ä¸´æ—¶ä¿®å¤NumPyå…¼å®¹æ€§é—®é¢˜
        original_zeros = np.zeros
        def patched_zeros(shape, dtype=float, order='C', **kwargs):
            # å¤„ç†å¯èƒ½çš„dtypeså‚æ•°ï¼ˆæŸäº›ç‰ˆæœ¬å¯èƒ½ä½¿ç”¨dtypesè€Œä¸æ˜¯dtypeï¼‰
            if 'dtypes' in kwargs:
                dtype = kwargs.pop('dtypes')
            return original_zeros(shape, dtype=dtype, order=order, **kwargs)
        np.zeros = patched_zeros
        
        try:
            from FCBF_module import FCBF
            fcbf_selector = FCBF()
            idx = fcbf_selector.fcbf(X_train_array, y_train_array, n_selected_features=top_k)
            idx = idx[:top_k]
        finally:
            # æ¢å¤åŸå§‹å‡½æ•°
            np.zeros = original_zeros
        
        selected_features = feature_labels[idx]
        importances_df = pd.DataFrame({'feature': selected_features})
        
    else:
        # å¦‚æœæ²¡æœ‰å¯ç”¨çš„FCBFåŒ…ï¼Œä½¿ç”¨fallback
        print(f"Warning: No FCBF package available, using first {top_k} features")
        idx = list(range(min(top_k, len(feature_labels))))
        selected_features = feature_labels[idx]
        importances_df = pd.DataFrame({'feature': selected_features})
    
    print(f"FCBF - å·²å®Œæˆ: {iteration}")
    return importances_df


def relief_feature_selection(X: pd.DataFrame, y: pd.Series,
                           feature_labels: np.ndarray,
                           iteration: int,
                           top_k: int = 50,
                           test_size: float = 0.3,
                           random_state: int = 42,
                           n_neighbors: int = 10) -> pd.DataFrame:
    """
    ä½¿ç”¨ReliefFè¿›è¡Œç‰¹å¾é€‰æ‹©
    """
    if not RELIEF_AVAILABLE:
        raise ImportError("skrebate not available")
        
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state
    )
    
    X_train_array = np.array(X_train)
    y_train_array = np.array(y_train)
    
    # å¯¼å…¥ReliefFï¼ˆåªæœ‰åœ¨å¯ç”¨æ—¶æ‰ä¼šæ‰§è¡Œåˆ°è¿™é‡Œï¼‰
    from skrebate import ReliefF
    fs = ReliefF(n_neighbors=n_neighbors)
    fs.fit(X_train_array, y_train_array)
    
    importances = fs.feature_importances_
    indices = np.argsort(importances)[::-1]
    top_indices = indices[:top_k]
    
    importances_df = pd.DataFrame({
        'feature': feature_labels[top_indices],
        'importance': importances[top_indices]
    })
    
    print(f"ReliefF - å·²å®Œæˆ: {iteration}")
    return importances_df


def mrmr_feature_selection(df: pd.DataFrame,
                          target_col: str,
                          iteration: int,
                          top_k: int = 50) -> pd.Series:
    """
    ä½¿ç”¨mRMRè¿›è¡Œç‰¹å¾é€‰æ‹©
    """
    if not PYMRMR_AVAILABLE:
        raise ImportError("pymrmr module not available")
    
    # mRMRéœ€è¦å®Œæ•´çš„æ•°æ®æ¡†
    mr = pymrmr.mRMR(df, 'MIQ', top_k)
    importances_df = pd.Series(mr)
    
    print(f"mRMR - å·²å®Œæˆ: {iteration}")
    return importances_df


def save_feature_selection_results(results_df, filename: str):
    """
    ä¿å­˜ç‰¹å¾é€‰æ‹©ç»“æœåˆ°æ–‡ä»¶
    """
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = Path(filename).parent
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # å¤„ç†ä¸åŒç±»å‹çš„ç»“æœ
    if isinstance(results_df, pd.DataFrame):
        if results_df.empty:
            print(f"Warning: Empty DataFrame for {filename}")
            # åˆ›å»ºä¸€ä¸ªç©ºçš„CSVæ–‡ä»¶
            pd.DataFrame(columns=['feature']).to_csv(filename, index=False)
        else:
            results_df.to_csv(filename, index=False)
    elif isinstance(results_df, pd.Series):
        # å¯¹äºmRMRç­‰è¿”å›Seriesçš„æ–¹æ³•
        results_df.to_csv(filename, index=False, header=['feature'])
    else:
        print(f"Warning: Unknown result type {type(results_df)} for {filename}")
        # å°è¯•è½¬æ¢ä¸ºDataFrame
        try:
            pd.DataFrame(results_df).to_csv(filename, index=False)
        except Exception as e:
            print(f"Error saving {filename}: {e}")
            # åˆ›å»ºç©ºæ–‡ä»¶
            pd.DataFrame(columns=['feature']).to_csv(filename, index=False)


def run_feature_selection_methods(input_dir: str = "resampling",
                                target_col: str = "dep_5",
                                iterations: int = 1000,
                                methods: list = None,
                                top_k: int = 50,
                                output_base_dir: str = "feature_selection_results",
                                id_col: str = None) -> dict:
    """
    è¿è¡Œæ‰€æœ‰ç‰¹å¾é€‰æ‹©æ–¹æ³•
    
    Args:
        input_dir: è¾“å…¥æ•°æ®ç›®å½•ï¼ˆresamplingæ–‡ä»¶å¤¹ï¼‰
        target_col: ç›®æ ‡åˆ—åï¼ˆç”¨æˆ·å®šä¹‰çš„target-colï¼‰
        iterations: è¿­ä»£æ¬¡æ•°ï¼ˆä¸feature_selection.pyä¸­çš„iterationsä¸€è‡´ï¼‰
        methods: è¦è¿è¡Œçš„æ–¹æ³•åˆ—è¡¨
        top_k: æ¯ä¸ªæ–¹æ³•é€‰æ‹©çš„ç‰¹å¾æ•°é‡ï¼ˆç”¨æˆ·å¯å®šä¹‰ï¼‰
        output_base_dir: è¾“å‡ºåŸºç¡€ç›®å½•
        id_col: IDåˆ—åï¼ˆç¬¬ä¸€åˆ—ï¼Œé€šå¸¸æ˜¯eidï¼‰
        
    Returns:
        dict: æ¯ä¸ªæ–¹æ³•çš„ç»“æœç»Ÿè®¡
    """
    if methods is None:
        methods = ['randomforest', 'elasticnet']
        if LIGHTGBM_AVAILABLE:
            methods.append('lightgbm')
        if RELIEF_AVAILABLE:
            methods.append('relief')
        if FCBF_AVAILABLE:
            methods.append('fcbf')
        if PYMRMR_AVAILABLE:
            methods.append('mrmr')
    
    # æ£€æŸ¥æ–¹æ³•å¯ç”¨æ€§
    available_methods = ['randomforest', 'elasticnet']
    if LIGHTGBM_AVAILABLE:
        available_methods.append('lightgbm')
    if RELIEF_AVAILABLE:
        available_methods.append('relief')
    if FCBF_AVAILABLE:
        available_methods.append('fcbf')
    if PYMRMR_AVAILABLE:
        available_methods.append('mrmr')
    
    # è¿‡æ»¤ä¸å¯ç”¨çš„æ–¹æ³•
    original_methods = methods.copy()
    methods = [m for m in methods if m in available_methods]
    
    # æ˜¾ç¤ºè¢«è¿‡æ»¤æ‰çš„æ–¹æ³•
    filtered_out = [m for m in original_methods if m not in methods]
    if filtered_out:
        print(f"âš ï¸  ä»¥ä¸‹æ–¹æ³•ä¸å¯ç”¨ï¼Œå·²è·³è¿‡: {', '.join(filtered_out)}")
        print(f"ğŸ’¡ å¯ç”¨çš„æ–¹æ³•: {', '.join(available_methods)}")
    
    if not methods:
        print("âŒ æŒ‡å®šçš„æ‰€æœ‰æ–¹æ³•éƒ½ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤æ–¹æ³•: randomforest, elasticnet")
        methods = ['randomforest', 'elasticnet']
    else:
        print(f"âœ… å°†è¿è¡Œä»¥ä¸‹æ–¹æ³•: {', '.join(methods)}")
    
    # æ–¹æ³•æ˜ å°„
    method_functions = {
        'lightgbm': lightgbm_feature_selection,
        'randomforest': randomforest_feature_selection,
        'elasticnet': elasticnet_feature_selection,
        'fcbf': fcbf_feature_selection,
        'relief': relief_feature_selection,
        'mrmr': mrmr_feature_selection
    }
    
    results_summary = {}
    
    for method in methods:
        if method not in method_functions:
            print(f"Warning: Unknown method '{method}', skipping...")
            continue
            
        print(f"\nå¼€å§‹è¿è¡Œ {method.upper()} ç‰¹å¾é€‰æ‹©...")
        method_results = []
        
        # åˆ›å»ºæ–¹æ³•ç‰¹å®šçš„è¾“å‡ºç›®å½•
        method_output_dir = Path(output_base_dir) / f"res_{method}_{iterations}"
        method_output_dir.mkdir(parents=True, exist_ok=True)
        
        for i in range(1, iterations + 1):
            input_file = Path(input_dir) / f"matched_Data_test{i}.csv"
            
            if not input_file.exists():
                print(f"Warning: File {input_file} not found, skipping...")
                continue
            
            # æ˜¾ç¤ºè¿›åº¦
            if i % 10 == 0 or i <= 5:
                print(f"  å¤„ç†æ–‡ä»¶ {i}/{iterations}...")
            
            try:
                if method == 'mrmr':
                    # mRMRéœ€è¦å®Œæ•´çš„æ•°æ®æ¡†
                    df = pd.read_csv(input_file)
                    if id_col:
                        df = df.drop(id_col, axis=1)
                    else:
                        df = df.drop(df.columns[0], axis=1)
                    
                    results = mrmr_feature_selection(df, target_col, i, top_k)
                else:
                    # å…¶ä»–æ–¹æ³•
                    X, y, feature_labels = acquire_data_for_feature_selection(input_file, target_col, id_col)
                    results = method_functions[method](X, y, feature_labels, i, top_k)
                
                # ä¿å­˜ç»“æœ
                output_file = method_output_dir / f"{method}_test_{i}.csv"
                if results is not None:
                    save_feature_selection_results(results, str(output_file))
                    method_results.append(str(output_file))
                else:
                    print(f"Warning: No results returned for {input_file} with {method}")
                
            except Exception as e:
                print(f"Error processing {input_file} with {method}: {e}")
                continue
        
        results_summary[method] = {
            'completed_files': len(method_results),
            'output_dir': str(method_output_dir)
        }
        
        print(f"{method.upper()} å®Œæˆ: {len(method_results)}/{iterations} æ–‡ä»¶")
    
    return results_summary


def analyze_robust_features(results_base_dir: str = "feature_selection_results", 
                          threshold: float = 0.5,
                          output_dir: str = "robust_features_analysis") -> Dict[str, Dict]:
    """
    åˆ†ææ¯ä¸ªç‰¹å¾é€‰æ‹©æ–¹æ³•çš„ç¨³å¥ç‰¹å¾
    
    Args:
        results_base_dir: ç‰¹å¾é€‰æ‹©ç»“æœçš„åŸºç¡€ç›®å½•
        threshold: ç‰¹å¾è¢«é€‰ä¸­çš„é¢‘ç‡é˜ˆå€¼ (0-1ä¹‹é—´)
        output_dir: è¾“å‡ºç›®å½•
        
    Returns:
        Dict: æ¯ä¸ªæ–¹æ³•çš„ç¨³å¥ç‰¹å¾ç»Ÿè®¡ç»“æœ
    """
    print("ğŸ” å¼€å§‹åˆ†æç¨³å¥ç‰¹å¾...")
    print("=" * 60)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # è·å–æ‰€æœ‰æ–¹æ³•çš„ç»“æœç›®å½•
    method_dirs = glob.glob(os.path.join(results_base_dir, "res_*"))
    
    if not method_dirs:
        print("âŒ æœªæ‰¾åˆ°ç‰¹å¾é€‰æ‹©ç»“æœç›®å½•")
        return {}
    
    robust_features_results = {}
    
    for method_dir in method_dirs:
        # æå–æ–¹æ³•åç§°
        method_name = os.path.basename(method_dir).replace("res_", "").split("_")[0]
        
        print(f"\nğŸ“Š åˆ†ææ–¹æ³•: {method_name}")
        print(f"   ç›®å½•: {method_dir}")
        
        # è·å–è¯¥æ–¹æ³•çš„æ‰€æœ‰CSVæ–‡ä»¶
        csv_files = glob.glob(os.path.join(method_dir, "*.csv"))
        
        if not csv_files:
            print(f"   âš ï¸  æœªæ‰¾åˆ°CSVæ–‡ä»¶")
            continue
            
        print(f"   ğŸ“ æ‰¾åˆ° {len(csv_files)} ä¸ªç»“æœæ–‡ä»¶")
        
        # ç»Ÿè®¡ç‰¹å¾é¢‘ç‡
        feature_counter = Counter()
        total_iterations = 0
        
        for csv_file in csv_files:
            try:
                df = pd.read_csv(csv_file)
                if 'feature' in df.columns:
                    features = df['feature'].dropna().tolist()
                    feature_counter.update(features)
                    total_iterations += 1
                else:
                    print(f"   âš ï¸  æ–‡ä»¶ {os.path.basename(csv_file)} ç¼ºå°‘ 'feature' åˆ—")
            except Exception as e:
                print(f"   âŒ è¯»å–æ–‡ä»¶ {os.path.basename(csv_file)} å¤±è´¥: {e}")
        
        if total_iterations == 0:
            print(f"   âŒ æ²¡æœ‰æœ‰æ•ˆçš„è¿­ä»£ç»“æœ")
            continue
            
        # è®¡ç®—ç‰¹å¾é¢‘ç‡å¹¶ç­›é€‰ç¨³å¥ç‰¹å¾
        feature_frequencies = {}
        robust_features = []
        
        for feature, count in feature_counter.items():
            frequency = count / total_iterations
            feature_frequencies[feature] = {
                'count': count,
                'frequency': frequency,
                'total_iterations': total_iterations
            }
            
            if frequency >= threshold:
                robust_features.append(feature)
        
        # æŒ‰é¢‘ç‡æ’åº
        sorted_features = sorted(feature_frequencies.items(), 
                               key=lambda x: x[1]['frequency'], 
                               reverse=True)
        
        print(f"   ğŸ“ˆ æ€»ç‰¹å¾æ•°: {len(feature_frequencies)}")
        print(f"   ğŸ¯ ç¨³å¥ç‰¹å¾æ•° (é¢‘ç‡ >= {threshold}): {len(robust_features)}")
        
        # ä¿å­˜ç»“æœ
        robust_features_results[method_name] = {
            'total_iterations': total_iterations,
            'total_features': len(feature_frequencies),
            'robust_features_count': len(robust_features),
            'robust_features': robust_features,
            'all_feature_frequencies': feature_frequencies,
            'threshold': threshold
        }
        
        # ä¿å­˜è¯¦ç»†ç»Ÿè®¡åˆ°CSV
        stats_df = pd.DataFrame([
            {
                'feature': feature,
                'count': stats['count'],
                'frequency': stats['frequency'],
                'is_robust': stats['frequency'] >= threshold
            }
            for feature, stats in sorted_features
        ])
        
        stats_file = os.path.join(output_dir, f"{method_name}_feature_statistics.csv")
        stats_df.to_csv(stats_file, index=False)
        print(f"   ğŸ’¾ ç»Ÿè®¡ç»“æœä¿å­˜åˆ°: {stats_file}")
        
        # ä¿å­˜ç¨³å¥ç‰¹å¾åˆ—è¡¨
        if robust_features:
            robust_df = pd.DataFrame({'robust_feature': robust_features})
            robust_file = os.path.join(output_dir, f"{method_name}_robust_features.csv")
            robust_df.to_csv(robust_file, index=False)
            print(f"   ğŸ’¾ ç¨³å¥ç‰¹å¾ä¿å­˜åˆ°: {robust_file}")
    
    # ä¿å­˜æ±‡æ€»æŠ¥å‘Š
    summary_data = []
    for method, results in robust_features_results.items():
        summary_data.append({
            'method': method,
            'total_iterations': results['total_iterations'],
            'total_features': results['total_features'],
            'robust_features_count': results['robust_features_count'],
            'robust_ratio': results['robust_features_count'] / results['total_features'] if results['total_features'] > 0 else 0,
            'threshold': results['threshold']
        })
    
    if summary_data:
        summary_df = pd.DataFrame(summary_data)
        summary_file = os.path.join(output_dir, "robust_features_summary.csv")
        summary_df.to_csv(summary_file, index=False)
        print(f"\nğŸ“‹ æ±‡æ€»æŠ¥å‘Šä¿å­˜åˆ°: {summary_file}")
    
    print(f"\nâœ… ç¨³å¥ç‰¹å¾åˆ†æå®Œæˆ! ç»“æœä¿å­˜åœ¨: {output_dir}")
    return robust_features_results



def create_final_feature_dataset(original_data_path: str,
                               analysis_dir: str,
                               output_path: str = "feature_selection_outputs/final_dataset.csv",
                               min_features: int = 10,
                               covariates: List[str] = None,
                               target_col: str = None,
                               id_col: str = None) -> bool:
    """
    æ ¹æ®æŠ•ç¥¨åˆ†æç»“æœåˆ›å»ºæœ€ç»ˆçš„ç‰¹å¾ç­›é€‰æ•°æ®é›†
    
    Args:
        original_data_path: åŸå§‹æ•°æ®æ–‡ä»¶è·¯å¾„
        analysis_dir: åˆ†æç»“æœç›®å½•è·¯å¾„
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        min_features: æœ€å°‘ç‰¹å¾æ•°é‡é˜ˆå€¼
        covariates: éœ€è¦ä¿ç•™çš„åå˜é‡åˆ—è¡¨
        target_col: ç›®æ ‡åˆ—å
        id_col: IDåˆ—å
        
    Returns:
        True if successful, False otherwise
    """
    
    try:
        print(f"\nğŸ¯ å¼€å§‹åˆ›å»ºæœ€ç»ˆç‰¹å¾æ•°æ®é›†...")
        print(f"ğŸ“ åŸå§‹æ•°æ®: {original_data_path}")
        print(f"ğŸ“Š åˆ†æç›®å½•: {analysis_dir}")
        
        # è¯»å–åŸå§‹æ•°æ®
        if not os.path.exists(original_data_path):
            print(f"âŒ åŸå§‹æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {original_data_path}")
            return False
            
        original_data = pd.read_csv(original_data_path)
        print(f"ğŸ“Š åŸå§‹æ•°æ®ç»´åº¦: {original_data.shape}")
        
        # æŸ¥æ‰¾æŠ•ç¥¨ç»“æœæ–‡ä»¶
        voting_files = []
        if os.path.exists(analysis_dir):
            for file in os.listdir(analysis_dir):
                if file.startswith("features_voted_by_") and file.endswith("_methods.csv"):
                    # æå–æ–¹æ³•æ•°é‡
                    try:
                        method_count = int(file.split("_")[3])
                        voting_files.append((method_count, file))
                    except (IndexError, ValueError):
                        continue
        
        if not voting_files:
            print(f"âŒ æœªæ‰¾åˆ°æŠ•ç¥¨ç»“æœæ–‡ä»¶åœ¨: {analysis_dir}")
            return False
        
        # æŒ‰æ–¹æ³•æ•°é‡æ’åºï¼Œé€‰æ‹©æœ€é«˜æŠ•ç¥¨æ•°ä¸”ç‰¹å¾æ•°>=min_featuresçš„ç»“æœ
        voting_files.sort(reverse=True)
        selected_features = []
        selected_method_count = 0
        
        for method_count, filename in voting_files:
            filepath = os.path.join(analysis_dir, filename)
            try:
                voting_df = pd.read_csv(filepath)
                if len(voting_df) >= min_features:
                    selected_features = voting_df['feature'].tolist()
                    selected_method_count = method_count
                    print(f"âœ… é€‰æ‹© {method_count} ä¸ªæ–¹æ³•æŠ•ç¥¨çš„ç»“æœ: {len(selected_features)} ä¸ªç‰¹å¾")
                    break
            except Exception as e:
                print(f"âš ï¸ è¯»å–æ–‡ä»¶å¤±è´¥ {filename}: {e}")
                continue
        
        if not selected_features:
            print(f"âŒ æœªæ‰¾åˆ°æ»¡è¶³æœ€å°‘ {min_features} ä¸ªç‰¹å¾çš„æŠ•ç¥¨ç»“æœ")
            return False
        
        # å‡†å¤‡æœ€ç»ˆçš„åˆ—åˆ—è¡¨
        final_columns = []
        
        # æ·»åŠ IDåˆ—
        if id_col and id_col in original_data.columns:
            final_columns.append(id_col)
            print(f"ğŸ“‹ åŒ…å«IDåˆ—: {id_col}")
        
        # æ·»åŠ ç›®æ ‡åˆ—
        if target_col and target_col in original_data.columns:
            final_columns.append(target_col)
            print(f"ğŸ¯ åŒ…å«ç›®æ ‡åˆ—: {target_col}")
        
        # æ·»åŠ åå˜é‡
        if covariates:
            available_covariates = [col for col in covariates if col in original_data.columns]
            final_columns.extend(available_covariates)
            print(f"ğŸ”§ åŒ…å«åå˜é‡: {available_covariates}")
        
        # æ·»åŠ é€‰ä¸­çš„ç‰¹å¾
        available_features = [feat for feat in selected_features if feat in original_data.columns]
        final_columns.extend(available_features)
        
        # å»é‡å¹¶ä¿æŒé¡ºåº
        final_columns = list(dict.fromkeys(final_columns))
        
        print(f"ğŸ“Š æœ€ç»ˆæ•°æ®é›†åŒ…å«:")
        print(f"   - IDåˆ—: {1 if id_col and id_col in final_columns else 0}")
        print(f"   - ç›®æ ‡åˆ—: {1 if target_col and target_col in final_columns else 0}")
        print(f"   - åå˜é‡: {len([c for c in (covariates or []) if c in final_columns])}")
        print(f"   - ç­›é€‰ç‰¹å¾: {len(available_features)}")
        print(f"   - æ€»åˆ—æ•°: {len(final_columns)}")
        
        # åˆ›å»ºæœ€ç»ˆæ•°æ®é›†
        final_dataset = original_data[final_columns].copy()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # ä¿å­˜æœ€ç»ˆæ•°æ®é›†
        final_dataset.to_csv(output_path, index=False)
        print(f"ğŸ’¾ æœ€ç»ˆæ•°æ®é›†ä¿å­˜åˆ°: {output_path}")
        
        # ä¿å­˜ç‰¹å¾é€‰æ‹©æŠ¥å‘Š
        report_path = output_path.replace('.csv', '_report.txt')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("ç‰¹å¾é€‰æ‹©æŠ¥å‘Š\n")
            f.write("="*50 + "\n\n")
            f.write(f"åŸå§‹æ•°æ®: {original_data_path}\n")
            f.write(f"åŸå§‹ç»´åº¦: {original_data.shape}\n")
            f.write(f"æœ€ç»ˆç»´åº¦: {final_dataset.shape}\n\n")
            f.write(f"é€‰æ‹©ç­–ç•¥: {selected_method_count} ä¸ªæ–¹æ³•æŠ•ç¥¨\n")
            f.write(f"æœ€å°‘ç‰¹å¾é˜ˆå€¼: {min_features}\n\n")
            f.write("åŒ…å«çš„åˆ—:\n")
            if id_col and id_col in final_columns:
                f.write(f"- IDåˆ—: {id_col}\n")
            if target_col and target_col in final_columns:
                f.write(f"- ç›®æ ‡åˆ—: {target_col}\n")
            if covariates:
                available_covs = [c for c in covariates if c in final_columns]
                if available_covs:
                    f.write(f"- åå˜é‡: {', '.join(available_covs)}\n")
            f.write(f"- ç­›é€‰ç‰¹å¾ ({len(available_features)}ä¸ª):\n")
            for i, feat in enumerate(available_features, 1):
                f.write(f"  {i:2d}. {feat}\n")
        
        print(f"ğŸ“‹ ç‰¹å¾é€‰æ‹©æŠ¥å‘Šä¿å­˜åˆ°: {report_path}")
        return True
        
    except Exception as e:
        print(f"âŒ åˆ›å»ºæœ€ç»ˆæ•°æ®é›†å¤±è´¥: {e}")
        return False


def analyze_feature_voting(results_base_dir: str = "feature_selection_results", 
                         threshold: float = 0.5,
                         output_dir: str = "feature_voting_analysis") -> Dict[str, List[str]]:
    """
    åˆ†æä¸åŒæ–¹æ³•é—´çš„ç‰¹å¾æŠ•ç¥¨æƒ…å†µ
    
    Args:
        results_base_dir: ç‰¹å¾é€‰æ‹©ç»“æœçš„åŸºç¡€ç›®å½•
        threshold: ç‰¹å¾è¢«é€‰ä¸­çš„é¢‘ç‡é˜ˆå€¼
        output_dir: è¾“å‡ºç›®å½•
        
    Returns:
        Dict: ä¸åŒæŠ•ç¥¨æ•°é‡å¯¹åº”çš„ç‰¹å¾é›†åˆ
    """
    print("\nğŸ—³ï¸  å¼€å§‹ç‰¹å¾æŠ•ç¥¨åˆ†æ...")
    print("=" * 60)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # é¦–å…ˆè·å–æ¯ä¸ªæ–¹æ³•çš„ç¨³å¥ç‰¹å¾
    print("ğŸ“Š è·å–å„æ–¹æ³•çš„ç¨³å¥ç‰¹å¾...")
    robust_results = analyze_robust_features(results_base_dir, threshold, 
                                           os.path.join(output_dir, "method_analysis"))
    
    if not robust_results:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ç¨³å¥ç‰¹å¾ç»“æœ")
        return {}
    
    # æ”¶é›†æ‰€æœ‰æ–¹æ³•çš„ç¨³å¥ç‰¹å¾
    method_robust_features = {}
    all_features = set()
    
    for method, results in robust_results.items():
        robust_features = set(results['robust_features'])
        method_robust_features[method] = robust_features
        all_features.update(robust_features)
        print(f"   {method}: {len(robust_features)} ä¸ªç¨³å¥ç‰¹å¾")
    
    print(f"\nğŸ”¢ æ€»è®¡å‘ç° {len(all_features)} ä¸ªå”¯ä¸€ç¨³å¥ç‰¹å¾")
    print(f"ğŸ“Š å‚ä¸æŠ•ç¥¨çš„æ–¹æ³•æ•°: {len(method_robust_features)}")
    
    # ç»Ÿè®¡æ¯ä¸ªç‰¹å¾è¢«å¤šå°‘ä¸ªæ–¹æ³•é€‰ä¸­
    feature_votes = defaultdict(list)
    
    for feature in all_features:
        voting_methods = []
        for method, features in method_robust_features.items():
            if feature in features:
                voting_methods.append(method)
        feature_votes[len(voting_methods)].append({
            'feature': feature,
            'methods': voting_methods
        })
    
    # æŒ‰æŠ•ç¥¨æ•°é‡åˆ†ç»„
    voting_results = {}
    
    print(f"\nğŸ—³ï¸  æŠ•ç¥¨ç»“æœç»Ÿè®¡:")
    print("-" * 40)
    
    for vote_count in sorted(feature_votes.keys(), reverse=True):
        features_info = feature_votes[vote_count]
        feature_names = [info['feature'] for info in features_info]
        voting_results[f"{vote_count}_methods"] = set(feature_names)
        
        print(f"ğŸ“Š {vote_count} ä¸ªæ–¹æ³•å…±åŒé€‰æ‹©: {len(feature_names)} ä¸ªç‰¹å¾")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        if features_info:
            vote_df = pd.DataFrame([
                {
                    'feature': info['feature'],
                    'vote_count': vote_count,
                    'voting_methods': ', '.join(info['methods'])
                }
                for info in features_info
            ])
            
            vote_file = os.path.join(output_dir, f"features_voted_by_{vote_count}_methods.csv")
            vote_df.to_csv(vote_file, index=False)
            print(f"   ğŸ’¾ ä¿å­˜åˆ°: {vote_file}")
            
            # æ˜¾ç¤ºå‰å‡ ä¸ªç‰¹å¾ä½œä¸ºç¤ºä¾‹
            if len(feature_names) <= 10:
                print(f"   ğŸ” ç‰¹å¾: {', '.join(feature_names)}")
            else:
                print(f"   ğŸ” å‰5ä¸ªç‰¹å¾: {', '.join(feature_names[:5])}...")
    
    # åˆ›å»ºæŠ•ç¥¨çŸ©é˜µ
    print(f"\nğŸ“Š åˆ›å»ºæ–¹æ³•-ç‰¹å¾æŠ•ç¥¨çŸ©é˜µ...")
    
    methods = list(method_robust_features.keys())
    features = sorted(all_features)
    
    # åˆ›å»ºæŠ•ç¥¨çŸ©é˜µ
    vote_matrix = []
    for feature in features:
        row = {'feature': feature}
        vote_count = 0
        for method in methods:
            voted = feature in method_robust_features[method]
            row[method] = 1 if voted else 0
            if voted:
                vote_count += 1
        row['total_votes'] = vote_count
        vote_matrix.append(row)
    
    # ä¿å­˜æŠ•ç¥¨çŸ©é˜µ
    matrix_df = pd.DataFrame(vote_matrix)
    matrix_df = matrix_df.sort_values('total_votes', ascending=False)
    matrix_file = os.path.join(output_dir, "feature_voting_matrix.csv")
    matrix_df.to_csv(matrix_file, index=False)
    print(f"ğŸ’¾ æŠ•ç¥¨çŸ©é˜µä¿å­˜åˆ°: {matrix_file}")
    
    # åˆ›å»ºæ±‡æ€»ç»Ÿè®¡
    summary_stats = []
    total_methods = len(methods)
    
    for vote_count in range(1, total_methods + 1):
        if vote_count in feature_votes:
            count = len(feature_votes[vote_count])
            percentage = (count / len(all_features)) * 100 if all_features else 0
            summary_stats.append({
                'vote_count': vote_count,
                'feature_count': count,
                'percentage': percentage,
                'description': f"{vote_count}/{total_methods} æ–¹æ³•ä¸€è‡´"
            })
    
    if summary_stats:
        summary_df = pd.DataFrame(summary_stats)
        summary_file = os.path.join(output_dir, "voting_summary.csv")
        summary_df.to_csv(summary_file, index=False)
        print(f"ğŸ’¾ æŠ•ç¥¨æ±‡æ€»ä¿å­˜åˆ°: {summary_file}")
    
    # ç‰¹åˆ«å…³æ³¨çš„ç»“æœ
    print(f"\nğŸ¯ å…³é”®å‘ç°:")
    print("-" * 30)
    
    if total_methods >= 2 and 2 in feature_votes:
        print(f"ğŸ¤ ä¸¤ä¸ªæ–¹æ³•å…±åŒé€‰æ‹©: {len(feature_votes[2])} ä¸ªç‰¹å¾")
    
    if total_methods >= 3 and 3 in feature_votes:
        print(f"ğŸ¤ ä¸‰ä¸ªæ–¹æ³•å…±åŒé€‰æ‹©: {len(feature_votes[3])} ä¸ªç‰¹å¾")
        
    if total_methods >= 4 and 4 in feature_votes:
        print(f"ğŸ¤ å››ä¸ªæ–¹æ³•å…±åŒé€‰æ‹©: {len(feature_votes[4])} ä¸ªç‰¹å¾")
    
    if total_methods in feature_votes:
        consensus_features = len(feature_votes[total_methods])
        print(f"ğŸ¯ æ‰€æœ‰æ–¹æ³•ä¸€è‡´é€‰æ‹©: {consensus_features} ä¸ªç‰¹å¾")
        if consensus_features > 0:
            print("   è¿™äº›æ˜¯æœ€ç¨³å¥çš„ç‰¹å¾!")
    
    print(f"\nâœ… ç‰¹å¾æŠ•ç¥¨åˆ†æå®Œæˆ! ç»“æœä¿å­˜åœ¨: {output_dir}")
    return voting_results

if __name__ == "__main__":
    # Example usage
    import argparse
    
    parser = argparse.ArgumentParser(description="Feature Selection Pipeline")
    parser.add_argument("--data", required=True,
                       help="Input data file path")
    parser.add_argument("--target-col", required=True,
                       help="Target column name")
    
    # ä¸å¹³è¡¡æ•°æ®é›†æµç¨‹å‚æ•°
    parser.add_argument("--iterations", type=int,
                       help="Number of iterations")
    parser.add_argument("--match-cols", nargs='+',
                       help="Columns to use for matching (space-separated)")
    parser.add_argument("--match-ratio", type=int, default=3,
                       help="Match ratio (default: 3)")
    parser.add_argument("--random-state", type=int, default=123,
                       help="Random state (default: 123)")
    
    # ç‰¹å¾é€‰æ‹©ç›¸å…³å‚æ•°
    parser.add_argument("--feature-methods", nargs='+', 
                       default=['randomforest', 'elasticnet'],
                       help="Feature selection methods to use")
    parser.add_argument("--top-k", type=int, default=50,
                       help="Number of top features to select (default: 50)")
    parser.add_argument("--id-col", default=None,
                       help="ID column name (optional)")
    
    # ç‰¹å¾åˆ†æç›¸å…³å‚æ•°
    parser.add_argument("--threshold", type=float, default=0.5,
                       help="Threshold for robust feature selection (default: 0.5)")
    parser.add_argument("--analysis-output", default="feature_analysis",
                       help="Output directory for analysis results")
    
    # æ•°æ®é›†å¹³è¡¡æ€§ç›¸å…³å‚æ•°
    parser.add_argument("--force-strategy", choices=['balanced', 'imbalanced', 'auto'],
                       default='auto', help="Force specific strategy (default: auto)")
    
    # æœ€ç»ˆæ•°æ®é›†ç”Ÿæˆå‚æ•°
    parser.add_argument("--min-features", type=int, default=10,
                       help="Minimum number of features for final dataset (default: 10)")
    parser.add_argument("--covariates", nargs='+', 
                       help="Covariates to keep in final dataset")
    
    args = parser.parse_args()
    
    # é¦–å…ˆåˆ†ææ•°æ®é›†å¹³è¡¡æ€§
    if not args.data or not args.target_col:
        parser.error("æ•°æ®é›†åˆ†æéœ€è¦ --data å’Œ --target-col å‚æ•°")
    
    # åˆ†ææ•°æ®é›†å¹³è¡¡æ€§ï¼ˆä½¿ç”¨å›ºå®šé˜ˆå€¼0.3ï¼‰
    balance_info = analyze_dataset_balance(args.data, args.target_col, balance_threshold=0.3)
    
    # å†³å®šä½¿ç”¨å“ªç§ç­–ç•¥
    if args.force_strategy == 'auto':
        strategy = balance_info['recommended_strategy']
    else:
        strategy = args.force_strategy
        print(f"\nğŸ”§ ç”¨æˆ·å¼ºåˆ¶æŒ‡å®šç­–ç•¥: {strategy}")
    
    print(f"\nğŸ¯ é‡‡ç”¨ç­–ç•¥: {strategy.upper()}")
    
    if strategy == 'balanced':
        # å¹³è¡¡æ•°æ®é›†ç­–ç•¥ï¼šç›´æ¥è¿›è¡Œç‰¹å¾é€‰æ‹©
        print("\n" + "="*60)
        print("ğŸ¯ æ‰§è¡Œå¹³è¡¡æ•°æ®é›†ç‰¹å¾é€‰æ‹©æµç¨‹")
        
        feature_results = run_balanced_feature_selection(
            data_path=args.data,
            target_col=args.target_col,
            methods=args.feature_methods,
            top_k=args.top_k,
            output_dir="feature_selection_outputs/feature_selection",
            id_col=args.id_col
        )
        
        # å¯¹å¹³è¡¡æ•°æ®é›†çš„ç»“æœè¿›è¡ŒæŠ•ç¥¨åˆ†æ
        if feature_results:
            print("\n" + "="*60)
            print("ğŸ—³ï¸ åˆ†æç‰¹å¾é€‰æ‹©ç»“æœçš„äº¤é›†...")
            
            # åˆ›å»ºä¸´æ—¶ç»“æœç›®å½•ç»“æ„ä¾›æŠ•ç¥¨åˆ†æä½¿ç”¨
            import shutil
            temp_results_dir = "temp_balanced_results"
            os.makedirs(temp_results_dir, exist_ok=True)
            
            # å¤åˆ¶ç»“æœæ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
            for method, result_file in feature_results.items():
                temp_method_dir = os.path.join(temp_results_dir, f"res_{method}_1")
                os.makedirs(temp_method_dir, exist_ok=True)
                
                # å¤åˆ¶æ–‡ä»¶
                shutil.copy2(result_file, os.path.join(temp_method_dir, f"{method}_test_1.csv"))
            
            # è¿›è¡ŒæŠ•ç¥¨åˆ†æï¼ˆé˜ˆå€¼è®¾ä¸º1.0ï¼Œå› ä¸ºåªæœ‰ä¸€æ¬¡é€‰æ‹©ï¼‰
            voting_results = analyze_feature_voting(
                results_base_dir=temp_results_dir,
                threshold=1.0,  # å¹³è¡¡æ•°æ®é›†åªåšä¸€æ¬¡é€‰æ‹©ï¼Œæ‰€ä»¥é˜ˆå€¼è®¾ä¸º1.0
                output_dir=get_feature_selection_subdir("analysis")
            )
            
            # æ¸…ç†ä¸´æ—¶ç›®å½•
            shutil.rmtree(temp_results_dir)
            
            print(f"\nğŸ‰ å¹³è¡¡æ•°æ®é›†åˆ†æå®Œæˆ!")
            print(f"ğŸ“ ç‰¹å¾é€‰æ‹©ç»“æœ: {get_feature_selection_subdir('feature_selection')}/")
            print(f"ğŸ“Š æŠ•ç¥¨åˆ†æç»“æœ: {get_feature_selection_subdir('analysis')}/")
            
            # åˆ›å»ºæœ€ç»ˆæ•°æ®é›†
            final_success = create_final_feature_dataset(
                original_data_path=args.data,
                analysis_dir=get_feature_selection_subdir("analysis"),
                output_path=os.path.join(get_feature_selection_dir(), "feature_selected_data.csv"),
                min_features=args.min_features,
                covariates=args.covariates,
                target_col=args.target_col,
                id_col=args.id_col
            )
            if final_success:
                print(f"ğŸ‰ æœ€ç»ˆæ•°æ®é›†åˆ›å»ºæˆåŠŸ!")
        
        exit(0)
    
    else:
        # ä¸å¹³è¡¡æ•°æ®é›†ç­–ç•¥ï¼šä½¿ç”¨åŸæœ‰çš„é‡é‡‡æ ·æµç¨‹
        print("\n" + "="*60)
        print("âš–ï¸ æ‰§è¡Œä¸å¹³è¡¡æ•°æ®é›†ç‰¹å¾é€‰æ‹©æµç¨‹ï¼ˆåŒ…å«é‡é‡‡æ ·ï¼‰")
        
        # éªŒè¯ä¸å¹³è¡¡æ•°æ®é›†æµç¨‹çš„å¿…éœ€å‚æ•°
        required_args = [ 'iterations', 'match_cols']
        missing_args = [arg for arg in required_args if getattr(args, arg.replace('-', '_')) is None]
        if missing_args:
            parser.error(f"ä¸å¹³è¡¡æ•°æ®é›†æµç¨‹éœ€è¦ä»¥ä¸‹å‚æ•°: {', '.join(['--' + arg for arg in missing_args])}")
        
        # è¿è¡Œæ•°æ®åŒ¹é…ç®¡é“ï¼ˆå›ºå®š75%é‡‡æ ·æ¯”ä¾‹ï¼‰
        success = run_feature_selection_pipeline(
            data_path=args.data,
            n_iterations=args.iterations,
            target_col=args.target_col,
            match_cols=args.match_cols,
            match_ratio=args.match_ratio,
            sample_ratio=0.75,  # å›ºå®š75%
            random_state=args.random_state
        )
        
        if success:
            print(f"\nâœ… Pipeline completed successfully!")
            print(f"Generated {args.iterations} matched datasets")
            print(f"Output directory: {get_feature_selection_subdir('resampling')}")
            
            # è¿è¡Œç‰¹å¾é€‰æ‹©æ–¹æ³•
            feature_results = run_feature_selection_methods(
                input_dir=get_feature_selection_subdir("resampling"),
                target_col=args.target_col,
                iterations=args.iterations,
                methods=args.feature_methods,
                top_k=args.top_k,
                output_base_dir=get_feature_selection_subdir("feature_selection"),
                id_col=args.id_col
            )
            
            print("\nâœ… ç‰¹å¾é€‰æ‹©å®Œæˆ!")
            for method, stats in feature_results.items():
                print(f"  {method}: {stats['completed_files']} æ–‡ä»¶ -> {stats['output_dir']}")
            
            # è‡ªåŠ¨è¿è¡Œç‰¹å¾åˆ†æ
            print("\n" + "="*60)
            print("ğŸ” å¼€å§‹è‡ªåŠ¨ç‰¹å¾åˆ†æ...")
            
            print("\nğŸ“Š æ­¥éª¤ 1: ç¨³å¥ç‰¹å¾åˆ†æ...")
            robust_results = analyze_robust_features(
                results_base_dir=get_feature_selection_subdir("feature_selection"),
                threshold=args.threshold,
                output_dir=os.path.join(get_feature_selection_subdir("analysis"), "robust_features")
            )
            
            print("\nğŸ—³ï¸ æ­¥éª¤ 2: ç‰¹å¾æŠ•ç¥¨åˆ†æ...")
            voting_results = analyze_feature_voting(
                results_base_dir=get_feature_selection_subdir("feature_selection"),
                threshold=args.threshold,
                output_dir=get_feature_selection_subdir("analysis")
            )
            
            print(f"\nğŸ‰ å®Œæ•´æµç¨‹å®Œæˆ!")
            print(f"ğŸ“ ç‰¹å¾é€‰æ‹©ç»“æœ: {get_feature_selection_subdir('feature_selection')}/")
            print(f"ğŸ“Š åˆ†æç»“æœ: {get_feature_selection_subdir('analysis')}/")
            
            # æ˜¾ç¤ºå…³é”®å‘ç°
            if voting_results and '6_methods' in voting_results:
                consensus_count = len(voting_results['6_methods'])
                if consensus_count > 0:
                    print(f"ğŸ¯ å‘ç° {consensus_count} ä¸ªæ‰€æœ‰æ–¹æ³•ä¸€è‡´é€‰æ‹©çš„æœ€ç¨³å¥ç‰¹å¾!")
            
            # åˆ›å»ºæœ€ç»ˆæ•°æ®é›†
            final_success = create_final_feature_dataset(
                original_data_path=args.data,
                analysis_dir=get_feature_selection_subdir("analysis"),
                output_path=os.path.join(get_feature_selection_dir(), "feature_selected_data.csv"),
                min_features=args.min_features,
                covariates=args.covariates,
                target_col=args.target_col,
                id_col=args.id_col
            )
            if final_success:
                print(f"ğŸ‰ æœ€ç»ˆæ•°æ®é›†åˆ›å»ºæˆåŠŸ!")
        
        exit(0 if success else 1)

