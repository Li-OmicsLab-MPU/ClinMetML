#!/usr/bin/env python3
"""
åŸºäºRFEçš„å‰å‘ç‰¹å¾é€‰æ‹©å’Œrefinementè„šæœ¬
ç”¨äºå¯¹æ’é™¤å…±çº¿æ€§åçš„æ•°æ®è¿›è¡Œç‰¹å¾é€‰æ‹©å’Œé‡è¦æ€§åˆ†æ

ä½œè€…: 
æ—¥æœŸ: 2025-11-13
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import Optional

# ClinMetML path management
from ..utils.paths import get_rfe_dir, get_multicollinearity_dir
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier
from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.feature_selection import RFE
from sklearn.metrics import roc_auc_score, classification_report
from imblearn.under_sampling import NearMiss
import matplotlib.colors as mcolors
from matplotlib.ticker import MaxNLocator
import warnings
import os
import json
from datetime import datetime
import argparse

warnings.filterwarnings('ignore')


def create_estimator(algorithm, **params):
    """
    æ ¹æ®ç®—æ³•åç§°åˆ›å»ºä¼°è®¡å™¨
    
    Parameters:
    -----------
    algorithm : str
        ç®—æ³•åç§°
    **params : dict
        ç®—æ³•å‚æ•°
        
    Returns:
    --------
    estimator : sklearn estimator
        åˆ›å»ºçš„ä¼°è®¡å™¨
    """
    estimators = {
        'logistic': LogisticRegression,
        'random_forest': RandomForestClassifier,
        'gradient_boosting': GradientBoostingClassifier,
        'ada_boost': AdaBoostClassifier,
        'extra_trees': ExtraTreesClassifier,
        'ridge': RidgeClassifier,
        'sgd': SGDClassifier,
        'svm': SVC,
        'knn': KNeighborsClassifier,
        'naive_bayes': GaussianNB,
        'decision_tree': DecisionTreeClassifier
    }
    
    if algorithm not in estimators:
        raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {algorithm}. æ”¯æŒçš„ç®—æ³•: {list(estimators.keys())}")
    
    return estimators[algorithm](**params)


def get_feature_importance(estimator, algorithm):
    """
    æ ¹æ®ç®—æ³•ç±»å‹è·å–ç‰¹å¾é‡è¦æ€§
    
    Parameters:
    -----------
    estimator : sklearn estimator
        è®­ç»ƒå¥½çš„ä¼°è®¡å™¨
    algorithm : str
        ç®—æ³•åç§°
        
    Returns:
    --------
    importance : array
        ç‰¹å¾é‡è¦æ€§æ•°ç»„
    """
    if hasattr(estimator, 'feature_importances_'):
        # æ ‘æ¨¡å‹å’Œé›†æˆæ–¹æ³•
        return estimator.feature_importances_
    elif hasattr(estimator, 'coef_'):
        # çº¿æ€§æ¨¡å‹
        return np.abs(estimator.coef_[0])
    elif algorithm == 'svm' and hasattr(estimator, 'dual_coef_'):
        # SVM (éœ€è¦linear kernel)
        if hasattr(estimator, 'coef_'):
            return np.abs(estimator.coef_[0])
        else:
            # å¯¹äºéçº¿æ€§SVMï¼Œè¿”å›å‡åŒ€é‡è¦æ€§
            return np.ones(estimator.n_features_in_) / estimator.n_features_in_
    else:
        # å…¶ä»–ç®—æ³•è¿”å›å‡åŒ€é‡è¦æ€§
        return np.ones(estimator.n_features_in_) / estimator.n_features_in_


class FeatureRefinementSelector:
    """
    åŸºäºRFEçš„ç‰¹å¾refinementå’Œå‰å‘é€‰æ‹©ç±»
    """
    
    def __init__(self, 
                 target_column,
                 algorithm='logistic',
                 test_size=0.3,
                 random_state=42,
                 sampling_strategy='auto',
                 estimator_params=None,
                 output_dir=None,
                 n_features_to_select=None,
                 id_column=None):
        """
        åˆå§‹åŒ–ç‰¹å¾é€‰æ‹©å™¨
        
        Parameters:
        -----------
        target_column : str
            ç›®æ ‡å˜é‡åˆ—å
        algorithm : str
            æœºå™¨å­¦ä¹ ç®—æ³•åç§°ï¼Œæ”¯æŒ: 'logistic', 'random_forest', 'gradient_boosting', 
            'ada_boost', 'extra_trees', 'ridge', 'sgd', 'svm', 'knn', 'naive_bayes', 'decision_tree'
        test_size : float
            æµ‹è¯•é›†æ¯”ä¾‹
        random_state : int
            éšæœºç§å­
        sampling_strategy : str
            é‡‡æ ·ç­–ç•¥: 'auto'(è‡ªåŠ¨åˆ¤æ–­), 'balanced'(å¼ºåˆ¶é‡‡æ ·), 'imbalanced'(ä¸é‡‡æ ·)
        estimator_params : dict
            ä¼°è®¡å™¨å‚æ•°
        output_dir : str
            è¾“å‡ºç›®å½•
        n_features_to_select : int, optional
            æœ€ç»ˆè¦é€‰æ‹©çš„ç‰¹å¾æ•°é‡ã€‚å¦‚æœä¸ºNoneï¼Œåˆ™ä½¿ç”¨æ‰€æœ‰ç‰¹å¾ã€‚
        id_column : str, optional
            IDåˆ—åç§°ï¼Œåœ¨ç‰¹å¾çŸ©é˜µä¸­å°†è¢«æ’é™¤ï¼Œä¸å‚ä¸RFEå’Œå»ºæ¨¡ã€‚
        """
        self.target_column = target_column
        self.algorithm = algorithm
        self.test_size = test_size
        self.random_state = random_state
        self.sampling_strategy = sampling_strategy
        self.output_dir = output_dir if output_dir is not None else get_rfe_dir()
        self.n_features_to_select = n_features_to_select
        self.id_column = id_column
        
        # æ ¹æ®ç®—æ³•è®¾ç½®é»˜è®¤å‚æ•°
        self.estimator_params = estimator_params or self._get_default_params(algorithm, random_state)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.output_dir, exist_ok=True)
        
        # åˆå§‹åŒ–æ•°æ®å±æ€§
        self.data = None
        self.X = None
        self.y = None
        self.X_resampled = None
        self.y_resampled = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.rfe_features = None
        self.selection_results = None
        
    def _get_default_params(self, algorithm, random_state):
        """
        æ ¹æ®ç®—æ³•è·å–é»˜è®¤å‚æ•°
        """
        default_params = {
            'logistic': {'random_state': random_state, 'max_iter': 1000, 'solver': 'liblinear'},
            'random_forest': {'random_state': random_state, 'n_estimators': 100, 'max_depth': None},
            'gradient_boosting': {'random_state': random_state, 'n_estimators': 100, 'learning_rate': 0.1},
            'ada_boost': {'random_state': random_state, 'n_estimators': 50, 'learning_rate': 1.0},
            'extra_trees': {'random_state': random_state, 'n_estimators': 100, 'max_depth': None},
            'ridge': {'random_state': random_state, 'alpha': 1.0},
            'sgd': {'random_state': random_state, 'max_iter': 1000, 'loss': 'log_loss'},
            'svm': {'random_state': random_state, 'kernel': 'linear', 'probability': True},
            'knn': {'n_neighbors': 5},
            'naive_bayes': {},
            'decision_tree': {'random_state': random_state, 'max_depth': None}
        }
        return default_params.get(algorithm, {'random_state': random_state})
        
    def load_data(self, data_path):
        """
        åŠ è½½æ•°æ®
        
        Parameters:
        -----------
        data_path : str
            æ•°æ®æ–‡ä»¶è·¯å¾„
        """
        print(f"ğŸ“‚ åŠ è½½æ•°æ®: {data_path}")
        self.data = pd.read_csv(data_path)
        print(f"æ•°æ®å½¢çŠ¶: {self.data.shape}")
        
        # æ£€æŸ¥ç›®æ ‡åˆ—æ˜¯å¦å­˜åœ¨
        if self.target_column not in self.data.columns:
            raise ValueError(f"ç›®æ ‡åˆ— '{self.target_column}' ä¸å­˜åœ¨äºæ•°æ®ä¸­")
        
        # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡å˜é‡
        feature_cols = [col for col in self.data.columns if col != self.target_column]
        if self.id_column is not None and self.id_column in feature_cols:
            feature_cols = [col for col in feature_cols if col != self.id_column]
            print(f"å¿½ç•¥IDåˆ—: {self.id_column}")

        self.X = self.data[feature_cols]
        self.y = self.data[self.target_column]
        
        print(f"ç‰¹å¾æ•°é‡: {self.X.shape[1]}")
        print(f"æ ·æœ¬æ•°é‡: {self.X.shape[0]}")
        print(f"ç›®æ ‡å˜é‡åˆ†å¸ƒ: {self.y.value_counts().to_dict()}")
        
    def apply_undersampling(self):
        """
        æ ¹æ®é‡‡æ ·ç­–ç•¥åº”ç”¨NearMissæ¬ é‡‡æ ·
        
        é‡‡æ ·ç­–ç•¥è¯´æ˜:
        - 'auto': è‡ªåŠ¨åˆ¤æ–­ï¼Œä¸å¹³è¡¡æ¯”ä¾‹>2.0æ—¶é‡‡æ ·
        - 'balanced': å¼ºåˆ¶è¿›è¡Œé‡‡æ ·
        - 'imbalanced': ä¸è¿›è¡Œé‡‡æ ·
        """
        print("ğŸ” åˆ†ææ•°æ®å¹³è¡¡æ€§...")
        
        # æ£€æŸ¥ç±»åˆ«åˆ†å¸ƒ
        class_counts = self.y.value_counts()
        print(f"åŸå§‹ç±»åˆ«åˆ†å¸ƒ: {class_counts.to_dict()}")
        
        # è®¡ç®—ä¸å¹³è¡¡æ¯”ä¾‹
        min_class_count = class_counts.min()
        max_class_count = class_counts.max()
        imbalance_ratio = max_class_count / min_class_count
        print(f"ä¸å¹³è¡¡æ¯”ä¾‹: {imbalance_ratio:.2f}")
        
        # æ ¹æ®ç­–ç•¥å†³å®šæ˜¯å¦é‡‡æ ·
        should_sample = self._should_apply_sampling(imbalance_ratio)
        
        if not should_sample:
            print("â­ï¸  è·³è¿‡é‡‡æ ·æ­¥éª¤")
            self.X_resampled = self.X.copy()
            self.y_resampled = self.y.copy()
            return
            
        print("ğŸ”„ åº”ç”¨NearMissæ¬ é‡‡æ ·...")
        np.random.seed(self.random_state)
        
        try:
            # åº”ç”¨NearMissé‡‡æ ·
            nr = NearMiss()
            self.X_resampled, self.y_resampled = nr.fit_resample(self.X, self.y)
            
            print(f"é‡‡æ ·åæ•°æ®å½¢çŠ¶: {self.X_resampled.shape}")
            print(f"é‡‡æ ·åç±»åˆ«åˆ†å¸ƒ: {pd.Series(self.y_resampled).value_counts().to_dict()}")
            
        except Exception as e:
            print(f"âš ï¸  é‡‡æ ·å¤±è´¥: {str(e)}")
            print("ä½¿ç”¨åŸå§‹æ•°æ®ç»§ç»­åˆ†æ...")
            self.X_resampled = self.X.copy()
            self.y_resampled = self.y.copy()
    
    def _should_apply_sampling(self, imbalance_ratio):
        """
        æ ¹æ®é‡‡æ ·ç­–ç•¥å’Œä¸å¹³è¡¡æ¯”ä¾‹åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡‡æ ·
        
        Parameters:
        -----------
        imbalance_ratio : float
            ä¸å¹³è¡¡æ¯”ä¾‹ (æœ€å¤§ç±»åˆ«æ•°é‡ / æœ€å°ç±»åˆ«æ•°é‡)
            
        Returns:
        --------
        bool : æ˜¯å¦åº”è¯¥è¿›è¡Œé‡‡æ ·
        """
        if self.sampling_strategy == 'balanced':
            print("ğŸ“‹ ç­–ç•¥: å¼ºåˆ¶é‡‡æ · (balanced)")
            return True
        elif self.sampling_strategy == 'imbalanced':
            print("ğŸ“‹ ç­–ç•¥: ä¸é‡‡æ · (imbalanced)")
            return False
        elif self.sampling_strategy == 'auto':
            print("ğŸ“‹ ç­–ç•¥: è‡ªåŠ¨åˆ¤æ–­ (auto)")
            # è‡ªåŠ¨åˆ¤æ–­ï¼šä¸å¹³è¡¡æ¯”ä¾‹å¤§äº2.0æ—¶é‡‡æ ·
            if imbalance_ratio >= 2.0:
                print(f"âœ… æ•°æ®ä¸å¹³è¡¡ (æ¯”ä¾‹: {imbalance_ratio:.2f} >= 2.0)ï¼Œå°†è¿›è¡Œé‡‡æ ·")
                return True
            else:
                print(f"âœ… æ•°æ®ç›¸å¯¹å¹³è¡¡ (æ¯”ä¾‹: {imbalance_ratio:.2f} < 2.0)ï¼Œè·³è¿‡é‡‡æ ·")
                return False
        else:
            print(f"âš ï¸  æœªçŸ¥çš„é‡‡æ ·ç­–ç•¥: {self.sampling_strategy}ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥(auto)")
            return imbalance_ratio >= 2.0
        
    def split_data(self):
        """
        åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        """
        print("ğŸ“Š åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†...")
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            self.X_resampled, 
            self.y_resampled, 
            test_size=self.test_size,
            random_state=self.random_state,
            stratify=self.y_resampled
        )
        
        print(f"è®­ç»ƒé›†å½¢çŠ¶: {self.X_train.shape}")
        print(f"æµ‹è¯•é›†å½¢çŠ¶: {self.X_test.shape}")
        
    def perform_rfe_ranking(self):
        """
        æ‰§è¡ŒRFEç‰¹å¾æ’å
        """
        print("ğŸ” æ‰§è¡ŒRFEç‰¹å¾æ’å...")
        
        # åˆ›å»ºä¼°è®¡å™¨
        estimator = create_estimator(self.algorithm, **self.estimator_params)
        
        # æ‰§è¡ŒRFE
        rfe = RFE(estimator=estimator, n_features_to_select=1, step=1)
        rfe.fit(self.X_train, self.y_train)
        
        # è·å–ç‰¹å¾æ’å
        feature_ranking = rfe.ranking_
        
        # æ„å»ºç‰¹å¾æ’åè¡¨
        self.rfe_features = pd.DataFrame({
            'Feature': self.X_train.columns,
            'Ranking': feature_ranking
        }).sort_values(by='Ranking')
        
        print(f"RFEæ’åå®Œæˆï¼Œå…± {len(self.rfe_features)} ä¸ªç‰¹å¾")
        print("å‰5ä¸ªç‰¹å¾:")
        print(self.rfe_features.head())
        
    def perform_forward_selection(self):
        """
        æ‰§è¡Œå‰å‘ç‰¹å¾é€‰æ‹©åˆ†æ
        """
        print("ğŸš€ æ‰§è¡Œå‰å‘ç‰¹å¾é€‰æ‹©åˆ†æ...")
        
        # ç¡®å®šè¦å¤„ç†çš„ç‰¹å¾æ•°é‡
        total_features = len(self.rfe_features)
        if self.n_features_to_select is not None:
            n_features = min(self.n_features_to_select, total_features)
            print(f"å°†æŒ‰ç…§RFEæ’åä¾æ¬¡é€‰æ‹©å‰ {n_features} ä¸ªç‰¹å¾")
        else:
            n_features = total_features
            print(f"å°†å¤„ç†æ‰€æœ‰ {n_features} ä¸ªç‰¹å¾")
        
        # åˆå§‹åŒ–ç»“æœDataFrame
        self.selection_results = pd.DataFrame(columns=['Feature', 'Importance', 'ROC'])
        selected_features = []
        
        # é€æ­¥æ·»åŠ ç‰¹å¾
        for i in range(n_features):
            # å½“å‰ç‰¹å¾
            current_feature = self.rfe_features.iloc[i]['Feature']
            selected_features.append(current_feature)
            
            # è®­ç»ƒæ¨¡å‹ï¼ˆä»…ä½¿ç”¨å½“å‰é€‰å®šçš„ç‰¹å¾ï¼‰
            X_train_subset = self.X_train[selected_features]
            X_test_subset = self.X_test[selected_features]
            
            # åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹
            model = create_estimator(self.algorithm, **self.estimator_params)
            model.fit(X_train_subset, self.y_train)
            
            # è·å–å½“å‰ç‰¹å¾çš„é‡è¦æ€§
            feature_importances = get_feature_importance(model, self.algorithm)
            importance = feature_importances[len(selected_features) - 1]
            
            # é¢„æµ‹å¹¶è®¡ç®—ROC AUCåˆ†æ•°
            y_pred_proba = model.predict_proba(X_test_subset)[:, 1]
            roc_score = roc_auc_score(self.y_test, y_pred_proba)
            
            # ä¿å­˜ç»“æœ
            new_row = pd.DataFrame({
                'Feature': [current_feature],
                'Importance': [importance],
                'ROC': [roc_score]
            })
            self.selection_results = pd.concat([self.selection_results, new_row], ignore_index=True)
            
            if (i + 1) % 5 == 0 or i == n_features - 1:
                print(f"å·²å¤„ç† {i + 1}/{n_features} ä¸ªç‰¹å¾ï¼Œå½“å‰AUC: {roc_score:.4f}")
        
        # å½’ä¸€åŒ–é‡è¦æ€§
        importance_sum = self.selection_results['Importance'].sum()
        if importance_sum > 0:
            self.selection_results['Importance_Normalized'] = (
                self.selection_results['Importance'] / importance_sum
            )
        else:
            # å¦‚æœé‡è¦æ€§å…¨ä¸º0ï¼Œä½¿ç”¨å‡åŒ€åˆ†å¸ƒ
            self.selection_results['Importance_Normalized'] = (
                np.ones(len(self.selection_results)) / len(self.selection_results)
            )
        
        print("å‰å‘ç‰¹å¾é€‰æ‹©å®Œæˆ!")
        print(f"æœ€ç»ˆAUC: {self.selection_results['ROC'].iloc[-1]:.4f}")
        
    def create_visualization(self, highlight_features=None, save_plot=True):
        """
        åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
        
        Parameters:
        -----------
        highlight_features : list
            éœ€è¦é«˜äº®æ˜¾ç¤ºçš„ç‰¹å¾åç§°åˆ—è¡¨
        save_plot : bool
            æ˜¯å¦ä¿å­˜å›¾è¡¨
        """
        print("ğŸ“Š åˆ›å»ºå¯è§†åŒ–å›¾è¡¨...")
        
        if self.selection_results is None:
            raise ValueError("è¯·å…ˆæ‰§è¡Œå‰å‘ç‰¹å¾é€‰æ‹©")
        
        # è®¾ç½®é«˜äº®ç‰¹å¾
        if highlight_features is None:
            highlight_features = []
        
        # åˆ›å»ºé¢œè‰²æ¸å˜ (é‡è¦æ€§é«˜çš„ç‰¹å¾é¢œè‰²æ·±)
        cmap = plt.get_cmap('Blues')
        norm = mcolors.PowerNorm(
            gamma=0.5, 
            vmin=self.selection_results['Importance_Normalized'].min(), 
            vmax=self.selection_results['Importance_Normalized'].max()
        )
        bar_colors = cmap(norm(self.selection_results['Importance_Normalized'].values))
        
        # å¼€å§‹ç»˜å›¾
        fig, ax1 = plt.subplots(figsize=(16, 8))
        fig.patch.set_facecolor('white')
        
        # ç»˜åˆ¶æŸ±çŠ¶å›¾ (Predictor Importance)
        ax1.bar(
            self.selection_results['Feature'], 
            self.selection_results['Importance_Normalized'], 
            color=bar_colors,
            label='Predictor Importance'
        )
        
        # è®¾ç½®å·¦ä¾§Yè½´ (ax1)
        ax1.set_ylabel('Predictor Importance', fontsize=12)
        
        # å®‰å…¨åœ°è®¾ç½®Yè½´é™åˆ¶
        max_importance = self.selection_results['Importance_Normalized'].max()
        if np.isfinite(max_importance) and max_importance > 0:
            ax1.set_ylim(0, max_importance * 1.1)
        else:
            ax1.set_ylim(0, 1.0)
            
        ax1.yaxis.set_major_locator(MaxNLocator(integer=False, nbins=8))
        ax1.tick_params(axis='y', labelsize=10)
        
        # è®¾ç½®Xè½´
        ax1.set_xlabel('Features', fontsize=12)
        ax1.tick_params(axis='x', labelsize=11)
        plt.setp(ax1.get_xticklabels(), rotation=45, ha='right')
        
        # åˆ›å»ºå…±äº«Xè½´çš„ç¬¬äºŒä¸ªYè½´ (ax2)
        ax2 = ax1.twinx()
        
        # ç»˜åˆ¶æŠ˜çº¿å›¾ (Cumulative AUC)
        ax2.plot(
            self.selection_results['Feature'], 
            self.selection_results['ROC'], 
            color='black', 
            marker='o', 
            linewidth=2,
            markersize=5,
            label='Cumulative AUC'
        )
        
        # è®¾ç½®å³ä¾§Yè½´ (ax2)
        ax2.set_ylabel('Cumulative AUC', fontsize=12)
        min_auc = self.selection_results['ROC'].min()
        max_auc = self.selection_results['ROC'].max()
        ax2.set_ylim(min_auc * 0.995, max_auc * 1.005)
        ax2.tick_params(axis='y', labelsize=10)
        
        # è°ƒæ•´Xè½´æ ‡ç­¾é¢œè‰²ï¼ˆé«˜äº®ç‰¹å®šç‰¹å¾ï¼‰
        for tick in ax1.get_xticklabels():
            if tick.get_text() in highlight_features:
                tick.set_color('red')
                tick.set_weight('bold')
        
        # æ·»åŠ ç½‘æ ¼å’Œæ ‡é¢˜
        plt.title('RFE-based Forward Feature Selection Results', fontsize=16, pad=20)
        ax1.grid(axis='y', linestyle='--', alpha=0.7)
        ax2.grid(axis='y', linestyle=':', alpha=0.5)
        
        # ä¼˜åŒ–å¸ƒå±€
        fig.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        if save_plot:
            plot_path = os.path.join(self.output_dir, f'feature_selection_plot_{self.algorithm}.png')
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            print(f"å›¾è¡¨å·²ä¿å­˜: {plot_path}")
        
        plt.show()
        
    def save_results(self, original_data_path=None, save_selected_data=False, selected_data_filename=None):
        """
        ä¿å­˜åˆ†æç»“æœå’Œé€‰æ‹©çš„ç‰¹å¾æ•°æ®
        
        Parameters:
        -----------
        original_data_path : str, optional
            åŸå§‹æ•°æ®æ–‡ä»¶è·¯å¾„ï¼Œç”¨äºä¿å­˜é€‰æ‹©çš„ç‰¹å¾æ•°æ®
        save_selected_data : bool
            æ˜¯å¦ä¿å­˜æœ€ç»ˆé€‰æ‹©çš„ç‰¹å¾æ•°æ®
        selected_data_filename : str, optional
            è¾“å‡ºæ–‡ä»¶åã€‚å¦‚æœä¸ºNoneï¼Œåˆ™è‡ªåŠ¨ç”Ÿæˆæ–‡ä»¶å
        """
        print("ğŸ’¾ ä¿å­˜åˆ†æç»“æœ...")
        
        # ä¿å­˜ç‰¹å¾é€‰æ‹©ç»“æœ
        results_path = os.path.join(self.output_dir, f'feature_selection_results_{self.algorithm}.csv')
        self.selection_results.to_csv(results_path, index=False)
        print(f"ç‰¹å¾é€‰æ‹©ç»“æœå·²ä¿å­˜: {results_path}")
        
        # ä¿å­˜RFEæ’åç»“æœ
        rfe_path = os.path.join(self.output_dir, f'rfe_ranking_{self.algorithm}.csv')
        self.rfe_features.to_csv(rfe_path, index=False)
        print(f"RFEæ’åç»“æœå·²ä¿å­˜: {rfe_path}")
        
        # ä¿å­˜åˆ†ææ‘˜è¦
        summary = {
            'target_column': self.target_column,
            'original_features': self.X.shape[1],
            'original_samples': self.X.shape[0],
            'final_auc': float(self.selection_results['ROC'].iloc[-1]),
            'best_features': self.selection_results.head(10)['Feature'].tolist(),
            'estimator_params': self.estimator_params,
            'sampling_strategy': self.sampling_strategy
        }
        
        summary_path = os.path.join(self.output_dir, f'analysis_summary_{self.algorithm}.json')
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        print(f"åˆ†ææ‘˜è¦å·²ä¿å­˜: {summary_path}")
        
        # ä¿å­˜æœ€ç»ˆé€‰æ‹©çš„ç‰¹å¾æ•°æ®ï¼ˆå¯é€‰ï¼‰
        if save_selected_data and original_data_path:
            print("ğŸ’¾ ä¿å­˜æœ€ç»ˆé€‰æ‹©çš„ç‰¹å¾æ•°æ®...")
            
            if self.selection_results is None:
                raise ValueError("è¯·å…ˆæ‰§è¡Œç‰¹å¾é€‰æ‹©åˆ†æ")
            
            # è¯»å–åŸå§‹æ•°æ®
            original_data = pd.read_csv(original_data_path)
            print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {original_data.shape}")
            
            # è·å–é€‰æ‹©çš„ç‰¹å¾åˆ—è¡¨
            selected_features = self.selection_results['Feature'].tolist()
            print(f"é€‰æ‹©çš„ç‰¹å¾æ•°é‡: {len(selected_features)}")
            
            # æ„å»ºæœ€ç»ˆåˆ—åˆ—è¡¨ï¼ˆåŒ…å«ç›®æ ‡åˆ—ï¼‰
            final_columns = [self.target_column] + selected_features
            
            # æ£€æŸ¥æ‰€æœ‰åˆ—æ˜¯å¦å­˜åœ¨
            missing_columns = [col for col in final_columns if col not in original_data.columns]
            if missing_columns:
                raise ValueError(f"ä»¥ä¸‹åˆ—åœ¨åŸå§‹æ•°æ®ä¸­ä¸å­˜åœ¨: {missing_columns}")
            
            # ç­›é€‰æ•°æ®
            selected_data = original_data[final_columns]
            print(f"ç­›é€‰åæ•°æ®å½¢çŠ¶: {selected_data.shape}")
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            if selected_data_filename is None:
                selected_data_filename = f'rfe_selected_data_{self.algorithm}.csv'
            
            output_path = os.path.join(self.output_dir, selected_data_filename)
            
            # ä¿å­˜æ•°æ®
            selected_data.to_csv(output_path, index=False)
            print(f"âœ… æœ€ç»ˆç‰¹å¾æ•°æ®å·²ä¿å­˜: {output_path}")
            print(f"åŒ…å«ç‰¹å¾: {selected_features}")
            
            return output_path
        
    def run_complete_analysis(self, data_path, highlight_features=None, save_selected_data=True, selected_data_filename=None):
        """
        è¿è¡Œå®Œæ•´çš„ç‰¹å¾refinementåˆ†æ
        
        Parameters:
        -----------
        data_path : str
            æ•°æ®æ–‡ä»¶è·¯å¾„
        highlight_features : list
            éœ€è¦é«˜äº®æ˜¾ç¤ºçš„ç‰¹å¾åç§°åˆ—è¡¨
        save_selected_data : bool
            æ˜¯å¦ä¿å­˜æœ€ç»ˆé€‰æ‹©çš„ç‰¹å¾æ•°æ®
        selected_data_filename : str, optional
            ä¿å­˜é€‰æ‹©ç‰¹å¾æ•°æ®çš„æ–‡ä»¶å
        """
        print("ğŸš€ å¼€å§‹å®Œæ•´çš„ç‰¹å¾refinementåˆ†æ...")
        print("=" * 60)
        
        try:
            # 1. åŠ è½½æ•°æ®
            self.load_data(data_path)
            
            # 2. åº”ç”¨é‡‡æ ·
            self.apply_undersampling()
            
            # 3. åˆ†å‰²æ•°æ®
            self.split_data()
            
            # 4. RFEæ’å
            self.perform_rfe_ranking()
            
            # 5. å‰å‘ç‰¹å¾é€‰æ‹©
            self.perform_forward_selection()
            
            # 6. åˆ›å»ºå¯è§†åŒ–
            self.create_visualization(highlight_features=highlight_features)
            
            # 7. ä¿å­˜ç»“æœå’Œé€‰æ‹©çš„ç‰¹å¾æ•°æ®
            self.save_results(
                original_data_path=data_path,
                save_selected_data=save_selected_data,
                selected_data_filename=selected_data_filename
            )
            
            print("=" * 60)
            print("âœ… ç‰¹å¾refinementåˆ†æå®Œæˆ!")
            print(f"æœ€ç»ˆæ¨¡å‹AUC: {self.selection_results['ROC'].iloc[-1]:.4f}")
            print(f"é€‰æ‹©çš„ç‰¹å¾æ•°é‡: {len(self.selection_results)}")
            print(f"ç»“æœå·²ä¿å­˜åˆ°: {self.output_dir}")
            
        except Exception as e:
            print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
            raise


class RFESelector:
    """Lightweight wrapper used by ClinMetMLPipeline for RFE selection.

    This class adapts FeatureRefinementSelector to the simpler interface
    expected by ClinMetMLPipeline.run_rfe_selection.
    """

    def __init__(self):
        self.last_results_dir: Optional[str] = None

    def select_features_rfe(
        self,
        data: pd.DataFrame,
        target_column: str,
        **kwargs,
    ) -> pd.DataFrame:
        """Run RFE-based refinement and return the selected-features DataFrame."""

        # Resolve output directory
        output_dir = kwargs.get("output_dir")
        if output_dir is None:
            output_dir = get_rfe_dir()
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        # Map estimator names from short codes to FeatureRefinementSelector algorithms
        estimator = kwargs.get("estimator", "rf")
        estimator_map = {
            "rf": "random_forest",
            "random_forest": "random_forest",
            "logistic": "logistic",
            "svm": "svm",
            "gb": "gradient_boosting",
            "gradient_boosting": "gradient_boosting",
            "et": "extra_trees",
            "extra_trees": "extra_trees",
            "dt": "decision_tree",
            "decision_tree": "decision_tree",
        }
        algorithm = estimator_map.get(estimator, "random_forest")

        n_features_to_select = kwargs.get("n_features_to_select", 20)
        id_column = kwargs.get("id_column")
        test_size = kwargs.get("test_size", 0.3)
        random_state = kwargs.get("random_state", 42)
        sampling_strategy = kwargs.get("sampling_strategy", "auto")

        # Persist current data to CSV so we can reuse the existing file-based API
        input_csv = output_path / "rfe_input_data.csv"
        data.to_csv(input_csv, index=False)

        selector = FeatureRefinementSelector(
            target_column=target_column,
            algorithm=algorithm,
            test_size=test_size,
            random_state=random_state,
            sampling_strategy=sampling_strategy,
            estimator_params=None,
            output_dir=str(output_path),
            n_features_to_select=n_features_to_select,
            id_column=id_column,
        )

        selector.run_complete_analysis(
            data_path=str(input_csv),
            highlight_features=None,
            save_selected_data=True,
            selected_data_filename="rfe_selected_data.csv",
        )

        # Load the final selected-features dataset
        final_path = output_path / "rfe_selected_data.csv"
        if not final_path.exists():
            raise FileNotFoundError(
                f"Expected RFE selected dataset at '{final_path}', but it was not found."
            )

        self.last_results_dir = str(output_path)
        selected_data = pd.read_csv(final_path)
        return selected_data


def main():
    """
    ä¸»å‡½æ•° - å‘½ä»¤è¡Œæ¥å£
    """
    parser = argparse.ArgumentParser(description='åŸºäºRFEçš„ç‰¹å¾refinementåˆ†æ')
    
    parser.add_argument('--data_path', type=str, 
                       default=None,
                       help='è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: auto-detected from multicollinearity reduction output)')
    parser.add_argument('--target_column', type=str, required=True,
                       help='ç›®æ ‡å˜é‡åˆ—å')
    parser.add_argument('--algorithm', type=str, default='random_forest',
                       choices=['logistic', 'random_forest', 'gradient_boosting', 'ada_boost', 
                               'extra_trees', 'ridge', 'sgd', 'svm', 'knn', 'naive_bayes', 'decision_tree'],
                       help='æœºå™¨å­¦ä¹ ç®—æ³• (é»˜è®¤: logistic)')
    parser.add_argument('--test_size', type=float, default=0.3,
                       help='æµ‹è¯•é›†æ¯”ä¾‹ (é»˜è®¤: 0.3)')
    parser.add_argument('--random_state', type=int, default=42,
                       help='éšæœºç§å­ (é»˜è®¤: 42)')
    parser.add_argument('--force_strategy', choices=['balanced', 'imbalanced', 'auto'],
                       default='auto', help='é‡‡æ ·ç­–ç•¥: auto(è‡ªåŠ¨åˆ¤æ–­), balanced(å¼ºåˆ¶é‡‡æ ·), imbalanced(ä¸é‡‡æ ·) (é»˜è®¤: auto)')
    parser.add_argument('--output_dir', type=str, default=None,
                       help='è¾“å‡ºç›®å½• (é»˜è®¤: auto-managed)')
    parser.add_argument('--highlight_features', type=str, nargs='*',
                       help='éœ€è¦é«˜äº®æ˜¾ç¤ºçš„ç‰¹å¾åç§°')
    parser.add_argument('--n_features_to_select', type=int, default=10,
                       help='æŒ‰ç…§RFEæ’åä¾æ¬¡é€‰æ‹©çš„ç‰¹å¾æ•°é‡ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™ä½¿ç”¨æ‰€æœ‰ç‰¹å¾ (é»˜è®¤: None)')
    parser.add_argument('--selected_data_filename', type=str, default=None,
                       help='ä¿å­˜é€‰æ‹©ç‰¹å¾æ•°æ®çš„æ–‡ä»¶åï¼Œå¦‚æœä¸æŒ‡å®šåˆ™è‡ªåŠ¨ç”Ÿæˆ')
    parser.add_argument('--id_column', type=str, default=None,
                       help='IDåˆ—åç§°ï¼Œå°†åœ¨ç‰¹å¾çŸ©é˜µä¸­è¢«æ’é™¤ï¼Œä¸å‚ä¸RFEåˆ†æ')
    
    args = parser.parse_args()
    
    # è®¾ç½®é»˜è®¤æ•°æ®è·¯å¾„
    if args.data_path is None:
        import os
        args.data_path = os.path.join(get_multicollinearity_dir(), "feature_selected_data_no_collinearity.csv")
    
    # æ„å»ºä¼°è®¡å™¨å‚æ•° (æ ¹æ®ç®—æ³•ç±»å‹)
    estimator_params = {'random_state': args.random_state}
    
    # åˆ›å»ºç‰¹å¾é€‰æ‹©å™¨
    selector = FeatureRefinementSelector(
        target_column=args.target_column,
        algorithm=args.algorithm,
        test_size=args.test_size,
        random_state=args.random_state,
        sampling_strategy=args.force_strategy,
        estimator_params=estimator_params,
        output_dir=args.output_dir,
        n_features_to_select=args.n_features_to_select,
        id_column=args.id_column
    )
    
    # è¿è¡Œåˆ†æ
    selector.run_complete_analysis(
        data_path=args.data_path,
        highlight_features=args.highlight_features,
        save_selected_data=True,
        selected_data_filename=args.selected_data_filename
    )


if __name__ == "__main__":
    main()
